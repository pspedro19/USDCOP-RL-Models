#!/bin/bash
# Script rÃ¡pido para fusionar datos histÃ³ricos usando mc y herramientas del sistema

echo "ðŸš€ FUSIÃ“N RÃPIDA DE DATOS HISTÃ“RICOS USDCOP"
echo "=============================================="

# Crear directorio temporal
TEMP_DIR="/tmp/usdcop_fusion"
mkdir -p "$TEMP_DIR"

echo "ðŸ“ Directorio temporal: $TEMP_DIR"

# Descargar todos los archivos CSV
echo "â¬‡ï¸ Descargando archivos CSV desde MinIO..."
docker exec usdcop-minio mc cp --recursive /data/00-raw-usdcop-marketdata/ "$TEMP_DIR/" > /dev/null 2>&1

# Encontrar archivos CSV vÃ¡lidos (no metadatos)
echo "ðŸ” Buscando archivos CSV vÃ¡lidos..."
CSV_FILES=$(docker exec usdcop-minio find "$TEMP_DIR" -name "*.csv" -type f | grep -v xl.meta | head -10)

if [ -z "$CSV_FILES" ]; then
    echo "âŒ No se encontraron archivos CSV vÃ¡lidos"
    exit 1
fi

echo "ðŸ“Š Archivos CSV encontrados:"
echo "$CSV_FILES"

# Crear un archivo Python temporal para fusiÃ³n
FUSION_SCRIPT="$TEMP_DIR/quick_fusion.py"

cat > "$FUSION_SCRIPT" << 'EOF'
import pandas as pd
import glob
import sys
import os
from datetime import datetime

print("ðŸ”„ Iniciando fusiÃ³n de archivos CSV...")

# Buscar archivos CSV
csv_files = []
for root, dirs, files in os.walk(sys.argv[1]):
    for file in files:
        if file.endswith('.csv') and 'xl.meta' not in file:
            csv_files.append(os.path.join(root, file))

print(f"ðŸ“‚ Encontrados {len(csv_files)} archivos CSV")

all_dfs = []
total_records = 0

for i, csv_file in enumerate(csv_files[:10], 1):  # Limitar a 10 archivos para prueba
    try:
        print(f"ðŸ“– [{i}/{min(len(csv_files), 10)}] Leyendo: {os.path.basename(csv_file)}")
        df = pd.read_csv(csv_file)

        # Renombrar columnas si es necesario
        if 'datetime' in df.columns:
            df = df.rename(columns={'datetime': 'time'})

        # Verificar columnas requeridas
        if 'time' in df.columns and 'close' in df.columns:
            print(f"   âœ… {len(df)} registros vÃ¡lidos")
            all_dfs.append(df)
            total_records += len(df)
        else:
            print(f"   âš ï¸ Columnas faltantes: {list(df.columns)}")

    except Exception as e:
        print(f"   âŒ Error: {e}")

if not all_dfs:
    print("âŒ No se encontraron datos vÃ¡lidos")
    sys.exit(1)

print(f"ðŸ”„ Fusionando {len(all_dfs)} DataFrames con {total_records:,} registros...")

# Fusionar todos los DataFrames
unified_df = pd.concat(all_dfs, ignore_index=True)

# Eliminar duplicados si existe columna 'time'
if 'time' in unified_df.columns:
    unified_df['time'] = pd.to_datetime(unified_df['time'])
    initial_count = len(unified_df)
    unified_df = unified_df.drop_duplicates(subset=['time']).sort_values('time')
    final_count = len(unified_df)
    print(f"âœ‚ï¸ Eliminados {initial_count - final_count:,} duplicados")

# Guardar resultado
output_file = f"{sys.argv[1]}/usdcop_unified_data.csv"
unified_df.to_csv(output_file, index=False)

print(f"ðŸ’¾ Dataset unificado guardado: {output_file}")
print(f"ðŸ“Š Total registros finales: {len(unified_df):,}")

if 'time' in unified_df.columns:
    print(f"ðŸ“… Rango: {unified_df['time'].min()} â†’ {unified_df['time'].max()}")

print("âœ… FusiÃ³n completada exitosamente")
EOF

# Ejecutar script de fusiÃ³n
echo "ðŸ Ejecutando script de fusiÃ³n..."
docker exec usdcop-minio python3 "$FUSION_SCRIPT" "$TEMP_DIR"

# Verificar resultado
UNIFIED_FILE="$TEMP_DIR/usdcop_unified_data.csv"
if docker exec usdcop-minio test -f "$UNIFIED_FILE"; then
    echo "âœ… Archivo unificado creado exitosamente"

    # Mostrar estadÃ­sticas del archivo
    echo "ðŸ“Š EstadÃ­sticas del archivo unificado:"
    docker exec usdcop-minio wc -l "$UNIFIED_FILE"
    docker exec usdcop-minio ls -lh "$UNIFIED_FILE"

    # Copiar archivo unificado a un location accesible
    docker exec usdcop-minio cp "$UNIFIED_FILE" /data/00-raw-usdcop-marketdata/UNIFIED_COMPLETE_DATA.csv
    echo "ðŸ“ Archivo copiado a: /data/00-raw-usdcop-marketdata/UNIFIED_COMPLETE_DATA.csv"

else
    echo "âŒ Error: No se pudo crear el archivo unificado"
    exit 1
fi

echo "ðŸŽ‰ PROCESO COMPLETADO"