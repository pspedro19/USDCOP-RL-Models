# =============================================================================
# DVC Pipeline Configuration (Phase 11 + MLOps)
# =============================================================================
# Data versioning and ML pipeline for USDCOP RL trading models.
# Updated with MLflow integration as part of MLOps-1/MLOps-2 remediation.
#
# Usage:
#   dvc repro                    # Run full pipeline
#   dvc repro prepare_data       # Run only data preparation
#   dvc repro train              # Run training with MLflow tracking
#   dvc dag                      # Show pipeline DAG
#   dvc metrics show             # Show all metrics
#   dvc plots show               # Generate plots
#
# Author: Trading Team
# Date: 2026-01-16
# =============================================================================

stages:
  # ===========================================================================
  # Stage 1: Prepare Training Data
  # ===========================================================================
  # Merges OHLCV and macro data into training-ready datasets.
  # Splits data into train/val/test sets with proper temporal ordering.
  prepare_data:
    cmd: python scripts/prepare_training_data.py
    deps:
      - data/raw/
      - scripts/prepare_training_data.py
      - config/feature_config.json
      - src/feature_store/builder.py
      - src/feature_store/calculators/
    params:
      - prepare.lookback_days
      - prepare.train_ratio
      - prepare.val_ratio
      - prepare.random_seed
    outs:
      - data/processed/:
          persist: true

  # ===========================================================================
  # Stage 2: Calculate Normalization Statistics
  # ===========================================================================
  # Computes mean/std for z-score normalization from training data only.
  calculate_norm_stats:
    cmd: >
      python -c "
      import pandas as pd
      import json
      from pathlib import Path

      # Load training dataset only (avoid data leakage)
      train_path = Path('data/processed/train_features.parquet')
      if train_path.exists():
          df = pd.read_parquet(train_path)
      else:
          df = pd.read_csv('data/pipeline/07_output/datasets_5min/RL_DS3_MACRO_CORE.csv')

      # Features to normalize
      features = ['log_ret_5m', 'log_ret_1h', 'log_ret_4h', 'rsi_9', 'atr_pct', 'adx_14',
                  'dxy_z', 'dxy_change_1d', 'vix_z', 'embi_z', 'brent_change_1d',
                  'rate_spread', 'usdmxn_change_1d']

      stats = {}
      for feat in features:
          if feat in df.columns:
              stats[feat] = {
                  'mean': float(df[feat].mean()),
                  'std': float(df[feat].std()),
                  'min': float(df[feat].min()),
                  'max': float(df[feat].max()),
                  'count': int(df[feat].count()),
                  'null_pct': float(df[feat].isna().mean())
              }

      with open('config/norm_stats.json', 'w') as f:
          json.dump(stats, f, indent=2)

      print(f'Generated norm_stats.json with {len(stats)} features')
      "
    deps:
      - data/processed/
      - config/feature_config.json
    outs:
      - config/norm_stats.json:
          cache: false

  # ===========================================================================
  # Stage 3: Train Model with MLflow
  # ===========================================================================
  # Trains PPO model with full MLflow tracking, metrics logging, and
  # automatic model registry integration.
  train:
    cmd: python scripts/train_with_mlflow.py --config params.yaml
    deps:
      - data/processed/
      - config/norm_stats.json
      - config/feature_config.json
      - scripts/train_with_mlflow.py
      - src/training/environments/
      - src/core/builders/
      - src/feature_store/core.py
    params:
      - train.learning_rate
      - train.n_steps
      - train.batch_size
      - train.n_epochs
      - train.gamma
      - train.gae_lambda
      - train.clip_range
      - train.ent_coef
      - train.total_timesteps
      - train.episode_length
    outs:
      - models/ppo_mlflow/:
          persist: true
    metrics:
      - reports/metrics.json:
          cache: false
    plots:
      - reports/training_curves.csv:
          x: step
          y: episode_reward_mean

  # ===========================================================================
  # Stage 4: Evaluate Model
  # ===========================================================================
  # Evaluates trained model on validation set.
  evaluate:
    cmd: python scripts/evaluate_model.py --model models/ppo_mlflow/
    deps:
      - models/ppo_mlflow/
      - data/processed/
      - config/norm_stats.json
      - scripts/evaluate_model.py
    params:
      - evaluate.n_episodes
      - evaluate.deterministic
    metrics:
      - reports/evaluation_metrics.json:
          cache: false
    plots:
      - reports/val_equity_curve.csv:
          x: step
          y: equity

  # ===========================================================================
  # Stage 5: Export to ONNX
  # ===========================================================================
  # Exports trained model to ONNX format for production inference.
  export_onnx:
    cmd: python scripts/export_to_onnx.py --model models/ppo_mlflow/ --output models/onnx/
    deps:
      - models/ppo_mlflow/
      - scripts/export_to_onnx.py
    params:
      - export.opset_version
      - export.optimize
    outs:
      - models/onnx/model.onnx:
          persist: true
      - models/onnx/model_metadata.json:
          persist: true

  # ===========================================================================
  # Stage 6: Backtest on Test Set
  # ===========================================================================
  # Final backtest on held-out test set. Run sparingly to avoid data leakage.
  backtest:
    cmd: python scripts/backtest.py --model models/onnx/ --mode test --output reports/backtest/
    deps:
      - models/onnx/model.onnx
      - data/processed/
      - config/norm_stats.json
      - scripts/backtest.py
      - config/trading_config.yaml
    params:
      - backtest.initial_balance
      - backtest.transaction_cost_bps
      - backtest.slippage_bps
    outs:
      - reports/backtest/trades.csv:
          persist: true
    metrics:
      - reports/backtest/metrics.json:
          cache: false
    plots:
      - reports/backtest/equity_curve.csv:
          x: timestamp
          y: equity
      - reports/backtest/drawdown.csv:
          x: timestamp
          y: drawdown_pct

  # ===========================================================================
  # Stage 7: Model Promotion Check
  # ===========================================================================
  # Checks if model meets promotion criteria and updates model registry.
  promote:
    cmd: >
      python scripts/promote_model.py
      --model-name usdcop-ppo-model
      --from-stage Staging
      --to-stage Production
      --reason "Automated promotion after successful backtest"
    deps:
      - reports/backtest/metrics.json
      - scripts/promote_model.py
    always_changed: true

  # ===========================================================================
  # FORECASTING PIPELINE STAGES
  # ===========================================================================
  # Daily USDCOP forecasting using ML models (XGBoost, LightGBM, etc.)
  # SSOT: src/forecasting/data_contracts.py, src/forecasting/config.py

  # ===========================================================================
  # Forecasting Stage 1: Build Daily Dataset
  # ===========================================================================
  # Fetches USDCOP daily data and builds SSOT-aligned features (19 features)
  forecast_prepare_data:
    cmd: python scripts/build_forecasting_dataset_aligned.py --start 2020-01-01 --output data/forecasting/aligned
    deps:
      - scripts/build_forecasting_dataset_aligned.py
      - src/forecasting/data_contracts.py
    params:
      - forecasting.data.train_start
      - forecasting.data.train_end
      - forecasting.features.num_features
    outs:
      - data/forecasting/aligned/:
          persist: true

  # ===========================================================================
  # Forecasting Stage 2: Train Models
  # ===========================================================================
  # Trains 9 models x 7 horizons = 63 model combinations with walk-forward CV
  forecast_train:
    cmd: >
      python -c "
      import sys
      sys.path.insert(0, '.')
      from src.forecasting.engine import ForecastingEngine
      from src.forecasting.contracts import ForecastingTrainingRequest, MODEL_IDS, HORIZONS
      from pathlib import Path
      import glob

      # Find latest dataset
      datasets = sorted(glob.glob('data/forecasting/aligned/*.parquet'))
      if not datasets:
          raise FileNotFoundError('No forecasting datasets found')

      dataset_path = datasets[-1]
      print(f'Using dataset: {dataset_path}')

      # Create training request
      request = ForecastingTrainingRequest(
          version='1.0.0',
          models=list(MODEL_IDS),
          horizons=list(HORIZONS),
          dataset_path=dataset_path,
          walk_forward_windows=5,
          minio_enabled=False,
          mlflow_enabled=True,
          experiment_name='forecasting-training',
      )

      # Train
      engine = ForecastingEngine(project_root=Path('.'))
      result = engine.train(request)

      print(f'Training completed: {result.models_trained}/{result.total_combinations}')
      print(f'Artifacts: {result.model_artifacts_path}')
      "
    deps:
      - data/forecasting/aligned/
      - src/forecasting/engine.py
      - src/forecasting/data_contracts.py
      - src/forecasting/contracts.py
      - src/forecasting/models/
      - src/forecasting/evaluation/
    params:
      - forecasting.training.walk_forward_windows
      - forecasting.training.random_seed
      - forecasting.training.models
    outs:
      - outputs/forecasting/:
          persist: true
    metrics:
      - outputs/forecasting/run_*/model_results.csv:
          cache: false

  # ===========================================================================
  # Forecasting Stage 3: Evaluate Models
  # ===========================================================================
  # Evaluates trained models and generates performance reports
  forecast_evaluate:
    cmd: >
      python -c "
      import pandas as pd
      import json
      from pathlib import Path
      import glob

      # Find latest run
      runs = sorted(glob.glob('outputs/forecasting/run_*'))
      if not runs:
          raise FileNotFoundError('No forecasting runs found')

      run_dir = Path(runs[-1])
      results_path = run_dir / 'model_results.csv'

      if not results_path.exists():
          raise FileNotFoundError(f'Results not found: {results_path}')

      df = pd.read_csv(results_path)

      # Calculate summary metrics
      summary = {
          'total_models': len(df),
          'avg_da': float(df['direction_accuracy'].mean()),
          'max_da': float(df['direction_accuracy'].max()),
          'min_da': float(df['direction_accuracy'].min()),
          'best_model': df.loc[df['direction_accuracy'].idxmax(), 'model_id'],
          'best_horizon': int(df.loc[df['direction_accuracy'].idxmax(), 'horizon']),
          'models_above_52': int((df['direction_accuracy'] > 52).sum()),
          'models_above_55': int((df['direction_accuracy'] > 55).sum()),
      }

      # Model rankings by average DA
      model_rankings = df.groupby('model_id')['direction_accuracy'].mean().sort_values(ascending=False)
      summary['model_rankings'] = model_rankings.to_dict()

      # Save evaluation report
      eval_dir = run_dir / 'evaluation'
      eval_dir.mkdir(exist_ok=True)

      with open(eval_dir / 'summary.json', 'w') as f:
          json.dump(summary, f, indent=2)

      print(f'Evaluation complete: {summary[\"total_models\"]} models')
      print(f'Best: {summary[\"best_model\"]} H={summary[\"best_horizon\"]} DA={summary[\"max_da\"]:.2f}%')
      "
    deps:
      - outputs/forecasting/
    metrics:
      - outputs/forecasting/run_*/evaluation/summary.json:
          cache: false

# =============================================================================
# Plots Configuration
# =============================================================================
plots:
  - reports/training_curves.csv:
      x: step
      y:
        - episode_reward_mean
        - policy_loss
        - value_loss
      title: "Training Progress"
  - reports/val_equity_curve.csv:
      x: step
      y: equity
      title: "Validation Equity Curve"
  - reports/backtest/equity_curve.csv:
      x: timestamp
      y: equity
      title: "Backtest Equity Curve"
  - reports/backtest/drawdown.csv:
      x: timestamp
      y: drawdown_pct
      title: "Drawdown Over Time"
  # Forecasting plots
  - outputs/forecasting/run_*/model_results.csv:
      x: horizon
      y: direction_accuracy
      title: "Direction Accuracy by Horizon"
