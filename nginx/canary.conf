# =============================================================================
# NGINX CANARY DEPLOYMENT CONFIGURATION
# =============================================================================
#
# This configuration enables canary deployments with weighted traffic splitting.
#
# Traffic Distribution:
# - Stable: 90% (default)
# - Canary: 10% (default)
#
# To adjust weights:
# 1. Edit the weights in the upstream block
# 2. Reload nginx: docker exec usdcop-nginx-canary nginx -s reload
#
# Contract: CTR-DEPLOY-002
# =============================================================================

worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/json;

    # Logging format with deployment tracking
    log_format canary '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_request_id" '
                      'upstream=$upstream_addr deployment=$upstream_http_x_deployment_type '
                      'rt=$request_time';

    access_log /var/log/nginx/access.log canary;

    # Performance optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_types text/plain text/css text/xml application/json application/javascript;

    # ==========================================================================
    # WEIGHTED UPSTREAM - CANARY TRAFFIC SPLIT
    # ==========================================================================
    #
    # Adjust weights to control traffic distribution:
    # - Start: 90% stable, 10% canary
    # - Gradual increase: 80/20, 70/30, 50/50
    # - Full promotion: 0% stable, 100% canary (then promote canary to stable)
    #
    upstream inference_canary_split {
        # Stable version - receives majority of traffic
        server inference-stable:8003 weight=90 max_fails=3 fail_timeout=30s;

        # Canary version - receives small percentage for testing
        server inference-canary:8003 weight=10 max_fails=3 fail_timeout=30s;

        keepalive 32;
    }

    # Stable-only upstream (for direct access/testing)
    upstream inference_stable_only {
        server inference-stable:8003 max_fails=3 fail_timeout=30s;
        keepalive 16;
    }

    # Canary-only upstream (for direct access/testing)
    upstream inference_canary_only {
        server inference-canary:8003 max_fails=3 fail_timeout=30s;
        keepalive 8;
    }

    # ==========================================================================
    # SERVER CONFIGURATION
    # ==========================================================================

    server {
        listen 80;
        server_name _;

        # Request ID for tracing
        set $request_id $http_x_request_id;
        if ($request_id = '') {
            set $request_id $request_id$connection$msec;
        }

        # Health check endpoint
        location /health {
            access_log off;
            return 200 '{"status":"healthy","service":"nginx-canary-lb"}';
            add_header Content-Type application/json;
        }

        # Canary status endpoint
        location /canary-status {
            access_log off;
            return 200 '{"mode":"canary","stable_weight":90,"canary_weight":10,"timestamp":"$time_iso8601"}';
            add_header Content-Type application/json;
        }

        # =======================================================================
        # PRODUCTION ENDPOINT - Uses Canary Split
        # =======================================================================

        location /v1/ {
            proxy_pass http://inference_canary_split;

            # Proxy headers
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Request-ID $request_id;
            proxy_set_header Connection "";

            # Capture deployment type from upstream response
            add_header X-Served-By $upstream_addr always;

            # Timeouts
            proxy_connect_timeout 10s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;

            # Error handling - fail to stable if canary fails
            proxy_next_upstream error timeout http_502 http_503 http_504;
            proxy_next_upstream_tries 2;
        }

        # =======================================================================
        # DIRECT ACCESS ENDPOINTS (for testing)
        # =======================================================================

        # Force stable only
        location /stable/ {
            rewrite ^/stable/(.*)$ /$1 break;
            proxy_pass http://inference_stable_only;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Request-ID $request_id;
            proxy_set_header Connection "";
        }

        # Force canary only
        location /canary/ {
            rewrite ^/canary/(.*)$ /$1 break;
            proxy_pass http://inference_canary_only;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Request-ID $request_id;
            proxy_set_header Connection "";
        }

        # Default route
        location / {
            return 404 '{"error":"not_found","message":"Use /v1/ for API endpoints"}';
            add_header Content-Type application/json;
        }
    }
}
