"""
L0 Data Acquisition Pipeline Contracts
=======================================

Pydantic contracts for Layer 0 (Data Acquisition) data flows.
Ensures type safety and validation for all L0 DAG operations.

Contract: CTR-L0-001

Architecture:
    External APIs → L0 (Data Acquisition) → PostgreSQL → L1 (Feature Calculation)

Data Sources:
    - TwelveData API: USD/COP OHLCV (5min bars)
    - FRED API: US economic indicators
    - BanRep SUAMECA: Colombian indicators
    - Investing.com: Market indices
    - BCRP Peru: EMBI spread

Version: 1.0.0
Author: Generated by Contract System
"""

from __future__ import annotations

import datetime as dt
from decimal import Decimal
from enum import Enum
from typing import Any, Dict, List, Literal, Optional, Union

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
    model_validator,
)


# =============================================================================
# ENUMS
# =============================================================================


class L0XComKeys(str, Enum):
    """XCom keys for L0 inter-task communication.

    Contract: CTR-L0-XCOM-001
    """
    # OHLCV Acquisition
    OHLCV_ROWS_INSERTED = "ohlcv_rows_inserted"
    OHLCV_COUNT = "ohlcv_count"
    LATEST_OHLCV_TIME = "latest_ohlcv_time"
    INVALID_TRADING_DATES = "invalid_trading_dates"

    # Macro Data
    FRED_DATA = "fred_data"
    TWELVEDATA_DATA = "twelvedata_data"
    INVESTING_DATA = "investing_data"
    BANREP_DATA = "banrep_data"
    BANREP_SDMX_DATA = "banrep_sdmx_data"  # BanRep REST API (P2-6)
    EMBI_DATA = "embi_data"
    FEDESARROLLO_DATA = "fedesarrollo_data"
    DANE_DATA = "dane_data"
    BANREP_BOP_DATA = "banrep_bop_data"  # BanRep Balance of Payments (quarterly)

    # Backup
    BACKUP_PATH = "backup_path"
    BACKUP_SIZE_MB = "backup_size_mb"
    BACKUP_TABLES = "backup_tables"


class DataSourceType(str, Enum):
    """External data source types."""
    TWELVEDATA = "twelvedata"
    FRED = "fred"
    BANREP = "banrep"
    BANREP_SDMX = "banrep_sdmx"  # BanRep REST API (P2-6)
    BANREP_BOP = "banrep_bop"  # BanRep Balance of Payments (quarterly)
    INVESTING = "investing"
    BCRP = "bcrp"  # Peru central bank for EMBI
    FEDESARROLLO = "fedesarrollo"  # Colombian think tank
    DANE = "dane"  # Colombian statistics agency


class AcquisitionStatus(str, Enum):
    """Status of data acquisition task."""
    SUCCESS = "success"
    ERROR = "error"
    SKIPPED = "skipped"
    PARTIAL = "partial"
    STALE = "stale"


class ExtractionOutcome(str, Enum):
    """Outcome of a single scraping/API extraction attempt.

    Contract: CTR-L0-EXTRACT-001
    """
    SUCCESS_NEW_DATA = "success_new_data"      # Data obtained, is newer than DB
    SUCCESS_SAME_DATA = "success_same_data"    # Data obtained, same as DB (no new publication)
    SUCCESS_PARTIAL = "success_partial"        # Some indicators obtained
    FAILURE_HTTP = "failure_http"              # HTTP error (4xx, 5xx)
    FAILURE_PARSE = "failure_parse"            # Response parsing failed
    FAILURE_TIMEOUT = "failure_timeout"        # Request timeout
    FAILURE_BLOCKED = "failure_blocked"        # Cloudflare/bot detection
    SKIPPED_NOT_PUBLISHED = "skipped_not_published"  # Known non-publication day


class PublicationSchedule(str, Enum):
    """Publication frequency for macro indicators.

    Contract: CTR-L0-SCHEDULE-001
    """
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    IRREGULAR = "irregular"


class IndicatorReadinessStatus(str, Enum):
    """Status of individual indicator readiness.

    Contract: CTR-L0-READINESS-001
    """
    FRESH = "fresh"             # New data obtained today
    FFILLED = "ffilled"         # Forward-filled within limits
    STALE = "stale"             # FFILLed beyond limits but usable
    MISSING = "missing"         # No data available
    ERROR = "error"             # Extraction failed


class TradingDayStatus(str, Enum):
    """Trading day validation status."""
    VALID = "valid"
    HOLIDAY = "holiday"
    WEEKEND = "weekend"
    OUTSIDE_HOURS = "outside_hours"


# =============================================================================
# INPUT CONTRACTS (External API Responses)
# =============================================================================


class TwelveDataBar(BaseModel):
    """Single bar from TwelveData API response.

    Contract: CTR-L0-INPUT-001
    """
    model_config = ConfigDict(frozen=True)

    datetime: str = Field(..., description="ISO timestamp from API")
    open: str = Field(..., description="Open price as string")
    high: str = Field(..., description="High price as string")
    low: str = Field(..., description="Low price as string")
    close: str = Field(..., description="Close price as string")
    volume: Optional[str] = Field(default="0", description="Volume (often 0 for FX)")

    def to_ohlcv_record(self, symbol: str = "USD/COP") -> "OHLCVRecord":
        """Convert to internal OHLCV record."""
        return OHLCVRecord(
            time=dt.datetime.fromisoformat(self.datetime),
            symbol=symbol,
            open=Decimal(self.open),
            high=Decimal(self.high),
            low=Decimal(self.low),
            close=Decimal(self.close),
            volume=int(float(self.volume)) if self.volume else 0,
            source=DataSourceType.TWELVEDATA.value
        )


class TwelveDataResponse(BaseModel):
    """TwelveData API time series response.

    Contract: CTR-L0-INPUT-002
    """
    meta: Optional[Dict[str, Any]] = Field(default=None)
    values: List[TwelveDataBar] = Field(default_factory=list)
    status: Optional[str] = Field(default=None)
    message: Optional[str] = Field(default=None)

    @property
    def is_valid(self) -> bool:
        """Check if response contains valid data."""
        return len(self.values) > 0 and self.status != "error"


class FREDSeriesValue(BaseModel):
    """Single observation from FRED API.

    Contract: CTR-L0-INPUT-003
    """
    model_config = ConfigDict(frozen=True)

    date: dt.date
    value: float
    series_id: str
    column_name: str  # Mapped column name in macro_indicators_daily


class MacroAPIResponse(BaseModel):
    """Generic macro API response.

    Contract: CTR-L0-INPUT-004
    """
    source: DataSourceType
    records: Dict[str, Dict[str, float]] = Field(
        default_factory=dict,
        description="Date string -> column name -> value"
    )
    errors: List[str] = Field(default_factory=list)
    status: AcquisitionStatus = AcquisitionStatus.SUCCESS

    @property
    def record_count(self) -> int:
        """Total number of date records."""
        return len(self.records)


# =============================================================================
# EXTRACTION TRACKING CONTRACTS
# =============================================================================


class ExtractionAttempt(BaseModel):
    """Single extraction attempt for one indicator/source.

    Contract: CTR-L0-EXTRACT-002
    Tracks individual indicator extraction attempts with outcome and timing.
    """
    model_config = ConfigDict(frozen=True)

    source: DataSourceType
    indicator_name: str = Field(..., description="Source identifier (e.g., FRED series ID)")
    column_name: str = Field(..., description="Target database column")
    outcome: ExtractionOutcome

    # Timing
    attempt_time: dt.datetime = Field(default_factory=dt.datetime.utcnow)
    duration_seconds: float = Field(default=0.0, ge=0)

    # Data comparison (when outcome is SUCCESS_*)
    latest_date_in_db: Optional[dt.date] = Field(
        default=None,
        description="Most recent date in DB before extraction"
    )
    latest_date_from_source: Optional[dt.date] = Field(
        default=None,
        description="Most recent date returned by source"
    )
    is_new_publication: bool = Field(
        default=False,
        description="True if source has newer data than DB"
    )

    # Error details (when outcome is FAILURE_*)
    error_code: Optional[int] = Field(default=None, description="HTTP status or error code")
    error_message: Optional[str] = Field(default=None, description="Error description")


class ExtractionBatchResult(BaseModel):
    """Aggregated result for a source's extraction batch.

    Contract: CTR-L0-EXTRACT-003
    Summary of all extraction attempts from a single source.
    """
    source: DataSourceType
    batch_start: dt.datetime = Field(default_factory=dt.datetime.utcnow)
    batch_end: Optional[dt.datetime] = Field(default=None)

    # Counts
    total_indicators: int = Field(default=0, ge=0)
    success_new: int = Field(default=0, ge=0, description="Indicators with new data")
    success_same: int = Field(default=0, ge=0, description="Indicators with same data")
    failures: int = Field(default=0, ge=0, description="Failed extractions")

    # Details
    attempts: List[ExtractionAttempt] = Field(default_factory=list)

    @property
    def success_rate(self) -> float:
        """Rate of successful extractions (new + same)."""
        if self.total_indicators == 0:
            return 0.0
        return (self.success_new + self.success_same) / self.total_indicators

    @property
    def new_data_rate(self) -> float:
        """Rate of indicators with new publications."""
        if self.total_indicators == 0:
            return 0.0
        return self.success_new / self.total_indicators

    @property
    def duration_seconds(self) -> float:
        """Total batch duration in seconds."""
        if self.batch_end is None:
            return 0.0
        return (self.batch_end - self.batch_start).total_seconds()

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for XCom serialization."""
        return {
            'source': self.source.value if isinstance(self.source, DataSourceType) else self.source,
            'batch_start': self.batch_start.isoformat() if self.batch_start else None,
            'batch_end': self.batch_end.isoformat() if self.batch_end else None,
            'total_indicators': self.total_indicators,
            'success_new': self.success_new,
            'success_same': self.success_same,
            'failures': self.failures,
            'success_rate': self.success_rate,
            'duration_seconds': self.duration_seconds,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ExtractionBatchResult":
        """Create from dictionary (XCom deserialization)."""
        return cls(
            source=DataSourceType(data['source']) if data.get('source') else DataSourceType.FRED,
            batch_start=dt.datetime.fromisoformat(data['batch_start']) if data.get('batch_start') else dt.datetime.utcnow(),
            batch_end=dt.datetime.fromisoformat(data['batch_end']) if data.get('batch_end') else None,
            total_indicators=data.get('total_indicators', 0),
            success_new=data.get('success_new', 0),
            success_same=data.get('success_same', 0),
            failures=data.get('failures', 0),
        )


# =============================================================================
# MACRO INDICATOR METADATA
# =============================================================================


class MacroIndicatorMetadata(BaseModel):
    """Metadata for a macro indicator including publication info.

    Contract: CTR-L0-META-001
    Defines publication schedule and FFILL limits per indicator.
    """
    model_config = ConfigDict(frozen=True)

    column_name: str = Field(..., description="Database column name")
    display_name: str = Field(..., description="Human-readable name")
    source: DataSourceType
    schedule: PublicationSchedule

    # Publication timing
    typical_publication_delay_days: int = Field(
        default=0,
        ge=0,
        description="Days after observation date until publication"
    )
    publication_time_utc: Optional[str] = Field(
        default=None,
        description="Typical publication time (e.g., '14:30')"
    )

    # FFILL limits based on schedule
    max_ffill_days: int = Field(
        default=5,
        ge=1,
        description="Maximum days to forward-fill"
    )


# =============================================================================
# MACRO INDICATOR REGISTRY - Generated from SSOT
# =============================================================================
# This registry is now dynamically generated from the SSOT
# (config/macro_variables_ssot.yaml) to prevent duplication.
# =============================================================================

def _build_macro_indicator_registry() -> Dict[str, MacroIndicatorMetadata]:
    """
    Build MACRO_INDICATOR_REGISTRY from SSOT.

    This function reads from the centralized SSOT file and generates
    the registry dynamically, ensuring consistency across all systems.
    """
    registry: Dict[str, MacroIndicatorMetadata] = {}

    # Try to import SSOT
    try:
        import sys
        from pathlib import Path

        # Add src to path if needed
        project_root = Path(__file__).parent.parent.parent.parent
        src_path = project_root / "src"
        if str(src_path) not in sys.path:
            sys.path.insert(0, str(src_path))

        from data.macro_ssot import MacroSSOT

        ssot = MacroSSOT()

        # Source type mapping
        source_mapping = {
            'fred': DataSourceType.FRED,
            'investing': DataSourceType.INVESTING,
            'twelvedata': DataSourceType.TWELVEDATA,
            'suameca': DataSourceType.BANREP,
            'bcrp': DataSourceType.BCRP,
            'fedesarrollo': DataSourceType.FEDESARROLLO,
            'dane': DataSourceType.DANE,
            'banrep_bop': DataSourceType.BANREP_BOP,
        }

        # Schedule mapping
        schedule_mapping = {
            'daily': PublicationSchedule.DAILY,
            'monthly': PublicationSchedule.MONTHLY,
            'quarterly': PublicationSchedule.QUARTERLY,
        }

        for var_name in ssot.get_all_variables():
            var_def = ssot.get_variable(var_name)
            if var_def is None:
                continue

            # Get source type
            source = source_mapping.get(
                var_def.extraction.primary_source,
                DataSourceType.FRED
            )

            # Get schedule
            schedule = schedule_mapping.get(
                var_def.identity.frequency,
                PublicationSchedule.DAILY
            )

            # Calculate delay days
            sched = var_def.schedule
            if sched.typical_day is not None:
                delay_days = sched.typical_day
            else:
                delay_days = sched.delay_days

            registry[var_name] = MacroIndicatorMetadata(
                column_name=var_name,
                display_name=var_def.display_name,
                source=source,
                schedule=schedule,
                typical_publication_delay_days=delay_days,
                max_ffill_days=var_def.ffill.max_days,
            )

        return registry

    except ImportError as e:
        # Fallback: return empty registry if SSOT not available
        import logging
        logging.getLogger(__name__).warning(
            f"Could not load SSOT, using empty registry: {e}"
        )
        return {}


# Generate the registry from SSOT
MACRO_INDICATOR_REGISTRY: Dict[str, MacroIndicatorMetadata] = _build_macro_indicator_registry()


# =============================================================================
# FFILL CONFIGURATION CONTRACTS
# =============================================================================


class FFillConfig(BaseModel):
    """Configuration for bounded forward-fill.

    Contract: CTR-L0-FFILL-001
    Defines maximum FFILL days by publication schedule.
    """
    model_config = ConfigDict(frozen=True)

    # Limits by schedule type
    daily_max_days: int = Field(default=5, ge=1, description="Max FFILL for daily indicators")
    weekly_max_days: int = Field(default=10, ge=1, description="Max FFILL for weekly indicators")
    monthly_max_days: int = Field(default=35, ge=1, description="Max FFILL for monthly indicators")
    quarterly_max_days: int = Field(default=95, ge=1, description="Max FFILL for quarterly indicators")

    # Behavior when limit exceeded
    on_limit_exceeded: Literal["null", "warn", "error"] = Field(
        default="warn",
        description="Action when FFILL exceeds limit"
    )

    def get_max_days_for_schedule(self, schedule: PublicationSchedule) -> int:
        """Get max FFILL days for a given publication schedule."""
        mapping = {
            PublicationSchedule.DAILY: self.daily_max_days,
            PublicationSchedule.WEEKLY: self.weekly_max_days,
            PublicationSchedule.MONTHLY: self.monthly_max_days,
            PublicationSchedule.QUARTERLY: self.quarterly_max_days,
            PublicationSchedule.IRREGULAR: self.daily_max_days,
        }
        return mapping.get(schedule, self.daily_max_days)


# Default FFILL configuration
DEFAULT_FFILL_CONFIG = FFillConfig()


class FFillApplicationResult(BaseModel):
    """Result of applying bounded FFILL.

    Contract: CTR-L0-FFILL-002
    Summary of forward-fill application with limit tracking.
    """
    config: FFillConfig = Field(default_factory=FFillConfig)
    timestamp: dt.datetime = Field(default_factory=dt.datetime.utcnow)

    # Counts per column
    columns_processed: int = Field(default=0, ge=0)
    rows_filled: int = Field(default=0, ge=0)
    rows_exceeded_limit: int = Field(default=0, ge=0)
    rows_set_to_null: int = Field(default=0, ge=0)

    # Details
    exceeded_columns: List[str] = Field(default_factory=list)
    fill_details: Dict[str, Dict[str, int]] = Field(
        default_factory=dict,
        description="column -> {filled: N, exceeded: M, max_days: D}"
    )

    @property
    def has_exceeded_limits(self) -> bool:
        """True if any column exceeded its FFILL limit."""
        return len(self.exceeded_columns) > 0


# =============================================================================
# INDICATOR READINESS CONTRACTS
# =============================================================================


class IndicatorReadiness(BaseModel):
    """Readiness status for a single indicator.

    Contract: CTR-L0-READINESS-002
    Tracks individual indicator status for daily readiness report.
    """
    model_config = ConfigDict(frozen=True)

    column_name: str
    display_name: str
    status: IndicatorReadinessStatus

    # Data info
    latest_value: Optional[float] = Field(default=None, description="Current value")
    latest_observation_date: Optional[dt.date] = Field(default=None, description="Date of observation")
    age_days: int = Field(default=0, ge=0, description="Days since last update")

    # Extraction info
    extraction_outcome: Optional[ExtractionOutcome] = Field(default=None)
    extraction_time: Optional[dt.datetime] = Field(default=None)

    # FFILL info
    is_ffilled: bool = Field(default=False)
    ffill_days: int = Field(default=0, ge=0)
    ffill_within_limit: bool = Field(default=True)


class DailyDataReadinessReport(BaseModel):
    """Comprehensive daily report confirming all data is ready for inference.

    Contract: CTR-L0-READINESS-003
    This is the SINGLE SOURCE OF TRUTH for data readiness.
    """
    # Report metadata
    report_date: dt.date = Field(default_factory=dt.date.today)
    report_time: dt.datetime = Field(default_factory=dt.datetime.utcnow)
    pipeline_execution_id: Optional[str] = Field(default=None)

    # Overall status
    is_ready_for_inference: bool = Field(default=False)
    readiness_score: float = Field(default=0.0, ge=0.0, le=1.0)

    # Extraction summary
    total_indicators: int = Field(default=31)
    indicators_fresh: int = Field(default=0, ge=0, description="Indicators with fresh data")
    indicators_ffilled: int = Field(default=0, ge=0, description="Indicators forward-filled within limits")
    indicators_stale: int = Field(default=0, ge=0, description="Indicators beyond FFILL limits")
    indicators_missing: int = Field(default=0, ge=0, description="Missing indicators")
    indicators_error: int = Field(default=0, ge=0, description="Failed extractions")

    # Source breakdown
    source_results: Dict[str, Any] = Field(default_factory=dict)

    # FFILL summary
    ffill_applied: bool = Field(default=False)
    ffill_total_rows: int = Field(default=0, ge=0)
    ffill_exceeded_limit: int = Field(default=0, ge=0)

    # Per-indicator details
    indicator_details: List[IndicatorReadiness] = Field(default_factory=list)

    # Blocking issues (if not ready)
    blocking_issues: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)

    # Critical indicators that must be present for inference
    CRITICAL_INDICATORS: List[str] = [
        "fxrt_index_dxy_usa_d_dxy",
        "volt_vix_usa_d_vix",
        "finc_bond_yield10y_usa_d_ust10y",
        "finc_bond_yield2y_usa_d_dgs2",
    ]

    @model_validator(mode="after")
    def compute_readiness(self) -> "DailyDataReadinessReport":
        """Compute overall readiness from indicator details."""
        issues = []
        warnings_list = []

        for detail in self.indicator_details:
            if detail.column_name in self.CRITICAL_INDICATORS:
                if detail.status in [IndicatorReadinessStatus.MISSING, IndicatorReadinessStatus.ERROR]:
                    issues.append(f"CRITICAL: {detail.display_name} is {detail.status.value}")
                elif detail.status == IndicatorReadinessStatus.STALE:
                    warnings_list.append(f"WARNING: {detail.display_name} is stale ({detail.age_days} days)")

        # Compute score: fresh=100%, ffilled=80%, stale=50%, missing/error=0%
        if self.total_indicators > 0:
            fresh_score = self.indicators_fresh / self.total_indicators
            ffilled_score = (self.indicators_ffilled * 0.8) / self.total_indicators
            stale_score = (self.indicators_stale * 0.5) / self.total_indicators
            object.__setattr__(self, 'readiness_score', fresh_score + ffilled_score + stale_score)

        # Ready if no blocking issues and score >= 70%
        object.__setattr__(self, 'blocking_issues', issues)
        object.__setattr__(self, 'warnings', warnings_list)
        is_ready = len(issues) == 0 and self.readiness_score >= 0.7
        object.__setattr__(self, 'is_ready_for_inference', is_ready)

        return self

    def to_summary_dict(self) -> Dict[str, Any]:
        """Return summary for logging/monitoring."""
        return {
            "date": str(self.report_date),
            "ready": self.is_ready_for_inference,
            "score": f"{self.readiness_score:.1%}",
            "fresh": self.indicators_fresh,
            "ffilled": self.indicators_ffilled,
            "stale": self.indicators_stale,
            "missing": self.indicators_missing,
            "errors": self.indicators_error,
            "blocking": len(self.blocking_issues),
        }


class MacroPipelineExecutionResult(BaseModel):
    """Complete execution result of the daily macro pipeline.

    Contract: CTR-L0-EXEC-001
    Aggregates all steps: extraction -> merge -> ffill -> validation -> readiness.
    """
    execution_id: str = Field(
        default_factory=lambda: f"macro-{dt.datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
    )
    start_time: dt.datetime = Field(default_factory=dt.datetime.utcnow)
    end_time: Optional[dt.datetime] = Field(default=None)
    duration_seconds: Optional[float] = Field(default=None)

    # Step results
    extraction_results: Dict[str, ExtractionBatchResult] = Field(default_factory=dict)
    merge_result: Optional["MacroMergeResult"] = Field(default=None)
    ffill_result: Optional[FFillApplicationResult] = Field(default=None)

    # Final readiness
    readiness_report: Optional[DailyDataReadinessReport] = Field(default=None)

    # Overall status
    pipeline_status: Literal["success", "partial", "failed"] = Field(default="failed")

    def finalize(self) -> "MacroPipelineExecutionResult":
        """Compute final status and duration."""
        self.end_time = dt.datetime.utcnow()
        self.duration_seconds = (self.end_time - self.start_time).total_seconds()

        if self.readiness_report and self.readiness_report.is_ready_for_inference:
            self.pipeline_status = "success"
        elif self.readiness_report and self.readiness_report.readiness_score > 0.5:
            self.pipeline_status = "partial"
        else:
            self.pipeline_status = "failed"

        return self


# =============================================================================
# OUTPUT CONTRACTS (Database Records)
# =============================================================================


class OHLCVRecord(BaseModel):
    """Single OHLCV bar record for database storage.

    Contract: CTR-L0-OUTPUT-001
    Target: usdcop_m5_ohlcv table
    """
    model_config = ConfigDict(frozen=True)

    time: dt.datetime = Field(..., description="Bar timestamp (COT timezone)")
    symbol: str = Field(default="USD/COP", description="Trading symbol")
    open: Decimal = Field(..., ge=0, description="Open price")
    high: Decimal = Field(..., ge=0, description="High price")
    low: Decimal = Field(..., ge=0, description="Low price")
    close: Decimal = Field(..., ge=0, description="Close price")
    volume: int = Field(default=0, ge=0, description="Volume")
    source: str = Field(default="twelvedata", description="Data source")

    @model_validator(mode="after")
    def validate_ohlc_consistency(self) -> "OHLCVRecord":
        """Ensure OHLC price consistency."""
        if self.high < self.low:
            raise ValueError(f"high ({self.high}) must be >= low ({self.low})")
        if self.high < self.open or self.high < self.close:
            raise ValueError("high must be >= open and close")
        if self.low > self.open or self.low > self.close:
            raise ValueError("low must be <= open and close")
        return self

    def is_in_market_hours(self, market_start: int = 8, market_end: int = 13) -> bool:
        """Check if bar is within Colombian market hours (8:00-12:55 COT)."""
        hour = self.time.hour
        minute = self.time.minute
        weekday = self.time.weekday()

        if weekday >= 5:  # Weekend
            return False
        if hour < market_start or hour >= market_end:
            return False
        if hour == market_end - 1 and minute > 55:
            return False
        return True


class MacroIndicatorRecord(BaseModel):
    """Single macro indicator record for database storage.

    Contract: CTR-L0-OUTPUT-002
    Target: macro_indicators_daily table
    """
    model_config = ConfigDict(frozen=True)

    fecha: dt.date = Field(..., description="Indicator date")

    # Dollar & FX (Daily)
    fxrt_index_dxy_usa_d_dxy: Optional[float] = Field(default=None, alias="dxy")
    fxrt_spot_usdmxn_mex_d_usdmxn: Optional[float] = Field(default=None, alias="usdmxn")
    fxrt_spot_usdclp_chl_d_usdclp: Optional[float] = Field(default=None, alias="usdclp")

    # Volatility & Risk
    volt_vix_usa_d_vix: Optional[float] = Field(default=None, alias="vix")
    crsk_spread_embi_col_d_embi: Optional[float] = Field(default=None, alias="embi")

    # Commodities
    comm_oil_wti_glb_d_wti: Optional[float] = Field(default=None, alias="wti")
    comm_oil_brent_glb_d_brent: Optional[float] = Field(default=None, alias="brent")
    comm_metal_gold_glb_d_gold: Optional[float] = Field(default=None, alias="gold")
    comm_agri_coffee_glb_d_coffee: Optional[float] = Field(default=None, alias="coffee")

    # US Rates
    finc_bond_yield10y_usa_d_ust10y: Optional[float] = Field(default=None, alias="ust10y")
    finc_bond_yield2y_usa_d_dgs2: Optional[float] = Field(default=None, alias="dgs2")
    polr_prime_rate_usa_d_prime: Optional[float] = Field(default=None, alias="prime")

    # Colombian Rates
    finc_rate_ibr_overnight_col_d_ibr: Optional[float] = Field(default=None, alias="ibr")
    polr_policy_rate_col_d_tpm: Optional[float] = Field(default=None, alias="tpm")
    finc_bond_yield10y_col_d_col10y: Optional[float] = Field(default=None, alias="col10y")
    finc_bond_yield5y_col_d_col5y: Optional[float] = Field(default=None, alias="col5y")

    # Colombian Equity
    eqty_index_colcap_col_d_colcap: Optional[float] = Field(default=None, alias="colcap")

    @property
    def non_null_count(self) -> int:
        """Count of non-null indicator values."""
        fields = [
            self.fxrt_index_dxy_usa_d_dxy,
            self.volt_vix_usa_d_vix,
            self.crsk_spread_embi_col_d_embi,
            self.comm_oil_brent_glb_d_brent,
            self.finc_bond_yield10y_usa_d_ust10y,
            self.finc_bond_yield2y_usa_d_dgs2,
            self.fxrt_spot_usdmxn_mex_d_usdmxn,
        ]
        return sum(1 for f in fields if f is not None)

    @property
    def is_complete_for_inference(self) -> bool:
        """Check if record has minimum required indicators for inference."""
        required = [
            self.fxrt_index_dxy_usa_d_dxy,
            self.volt_vix_usa_d_vix,
            self.finc_bond_yield10y_usa_d_ust10y,
            self.finc_bond_yield2y_usa_d_dgs2,
        ]
        return all(v is not None for v in required)


# =============================================================================
# TASK RESULT CONTRACTS
# =============================================================================


class OHLCVAcquisitionResult(BaseModel):
    """Result of OHLCV acquisition task.

    Contract: CTR-L0-TASK-001
    """
    status: AcquisitionStatus
    rows_inserted: int = Field(default=0, ge=0)
    rows_filtered: int = Field(default=0, ge=0, description="Bars outside market hours")
    timestamp: dt.datetime = Field(default_factory=dt.datetime.utcnow)
    reason: Optional[str] = Field(default=None)

    @property
    def is_successful(self) -> bool:
        return self.status == AcquisitionStatus.SUCCESS


class MacroAcquisitionResult(BaseModel):
    """Result of macro data acquisition task.

    Contract: CTR-L0-TASK-002
    """
    source: DataSourceType
    status: AcquisitionStatus
    dates_acquired: int = Field(default=0, ge=0)
    columns_updated: int = Field(default=0, ge=0)
    errors: List[str] = Field(default_factory=list)
    timestamp: dt.datetime = Field(default_factory=dt.datetime.utcnow)


class MacroMergeResult(BaseModel):
    """Result of merging all macro sources.

    Contract: CTR-L0-TASK-003
    """
    status: AcquisitionStatus
    total_dates: int = Field(default=0, ge=0)
    total_updated: int = Field(default=0, ge=0)
    sources_merged: List[DataSourceType] = Field(default_factory=list)
    forward_fill_count: int = Field(default=0, ge=0)
    timestamp: dt.datetime = Field(default_factory=dt.datetime.utcnow)


class DataValidationResult(BaseModel):
    """Result of data validation task.

    Contract: CTR-L0-TASK-004
    """
    status: AcquisitionStatus
    total_bars: int = Field(default=0, ge=0)
    latest_time: Optional[dt.datetime] = Field(default=None)
    invalid_dates: List[Dict[str, str]] = Field(
        default_factory=list,
        description="List of dates with data on non-trading days"
    )

    @property
    def is_valid(self) -> bool:
        return self.status == AcquisitionStatus.SUCCESS and self.total_bars > 0


class BackupResult(BaseModel):
    """Result of backup task.

    Contract: CTR-L0-TASK-005
    """
    status: AcquisitionStatus
    backup_path: Optional[str] = Field(default=None)
    size_mb: float = Field(default=0.0, ge=0)
    tables_backed_up: List[str] = Field(default_factory=list)
    timestamp: dt.datetime = Field(default_factory=dt.datetime.utcnow)

    @property
    def is_successful(self) -> bool:
        return self.status == AcquisitionStatus.SUCCESS and self.backup_path is not None


class GapDetectionResult(BaseModel):
    """Result of gap detection in OHLCV data.

    Contract: CTR-L0-TASK-006
    """
    gaps_found: int = Field(default=0, ge=0)
    gap_ranges: List[Dict[str, str]] = Field(
        default_factory=list,
        description="List of gap ranges with start/end times"
    )
    bars_backfilled: int = Field(default=0, ge=0)
    timestamp: dt.datetime = Field(default_factory=dt.datetime.utcnow)

    @property
    def has_gaps(self) -> bool:
        return self.gaps_found > 0


# =============================================================================
# CONFIGURATION CONTRACTS
# =============================================================================


class TwelveDataConfig(BaseModel):
    """TwelveData API configuration.

    Contract: CTR-L0-CONFIG-001
    """
    api_key: str = Field(..., min_length=10)
    symbol: str = Field(default="USD/COP")
    interval: str = Field(default="5min")
    timezone: str = Field(default="America/Bogota")
    output_size: int = Field(default=100, ge=1, le=5000)


class FREDConfig(BaseModel):
    """FRED API configuration.

    Contract: CTR-L0-CONFIG-002
    """
    api_key: str = Field(..., min_length=10)
    series_mapping: Dict[str, str] = Field(
        default_factory=dict,
        description="FRED series ID -> database column mapping"
    )
    lookback_days: int = Field(default=90, ge=1, le=365)


class MarketHoursConfig(BaseModel):
    """Market hours configuration.

    Contract: CTR-L0-CONFIG-003
    """
    timezone: str = Field(default="America/Bogota")
    local_start: str = Field(default="08:00")
    local_end: str = Field(default="12:55")
    trading_days: List[int] = Field(
        default=[0, 1, 2, 3, 4],
        description="Weekdays (0=Monday, 4=Friday)"
    )

    @property
    def start_hour(self) -> int:
        return int(self.local_start.split(":")[0])

    @property
    def end_hour(self) -> int:
        return int(self.local_end.split(":")[0])


# =============================================================================
# QUALITY REPORT CONTRACTS
# =============================================================================


class L0QualityReport(BaseModel):
    """L0 data quality report.

    Contract: CTR-L0-REPORT-001
    """
    report_date: dt.date = Field(default_factory=dt.date.today)

    # OHLCV Quality
    ohlcv_bar_count: int = Field(default=0)
    ohlcv_latest_time: Optional[dt.datetime] = Field(default=None)
    ohlcv_gaps_count: int = Field(default=0)

    # Macro Quality
    macro_coverage: Dict[str, bool] = Field(
        default_factory=dict,
        description="Column name -> has data today"
    )
    macro_staleness_days: Dict[str, int] = Field(
        default_factory=dict,
        description="Column name -> days since last update"
    )

    # Overall
    is_production_ready: bool = Field(default=False)
    issues: List[str] = Field(default_factory=list)

    @model_validator(mode="after")
    def validate_production_readiness(self) -> "L0QualityReport":
        """Determine if data is production ready."""
        issues = []

        if self.ohlcv_bar_count == 0:
            issues.append("No OHLCV data available")

        if self.ohlcv_gaps_count > 0:
            issues.append(f"{self.ohlcv_gaps_count} gaps in OHLCV data")

        # Check critical macro columns
        critical_columns = [
            "fxrt_index_dxy_usa_d_dxy",
            "volt_vix_usa_d_vix",
            "finc_bond_yield10y_usa_d_ust10y",
        ]
        for col in critical_columns:
            if not self.macro_coverage.get(col, False):
                issues.append(f"Missing critical macro: {col}")

        self.issues = issues
        self.is_production_ready = len(issues) == 0
        return self


# =============================================================================
# FACTORY FUNCTIONS
# =============================================================================


def create_ohlcv_acquisition_result(
    rows_inserted: int,
    rows_filtered: int = 0,
    reason: Optional[str] = None
) -> OHLCVAcquisitionResult:
    """Factory for OHLCV acquisition result."""
    status = AcquisitionStatus.SUCCESS if rows_inserted > 0 else AcquisitionStatus.SKIPPED
    return OHLCVAcquisitionResult(
        status=status,
        rows_inserted=rows_inserted,
        rows_filtered=rows_filtered,
        reason=reason
    )


def create_macro_acquisition_result(
    source: DataSourceType,
    dates_acquired: int,
    columns_updated: int = 0,
    errors: Optional[List[str]] = None
) -> MacroAcquisitionResult:
    """Factory for macro acquisition result."""
    if errors and len(errors) > 0:
        status = AcquisitionStatus.PARTIAL if dates_acquired > 0 else AcquisitionStatus.ERROR
    else:
        status = AcquisitionStatus.SUCCESS if dates_acquired > 0 else AcquisitionStatus.SKIPPED

    return MacroAcquisitionResult(
        source=source,
        status=status,
        dates_acquired=dates_acquired,
        columns_updated=columns_updated,
        errors=errors or []
    )
