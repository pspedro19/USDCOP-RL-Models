# PostgreSQL Database Enhancements Guide
## USDCOP Trading System L0 Pipeline

### Overview
This guide documents the enhanced PostgreSQL database integration for the L0 pipeline system, including new tables, functionality, and optimization features.

## ðŸ—ƒï¸ Enhanced Database Schema

### New Tables Added

#### 1. `backup_metadata`
Tracks backup files, freshness, and completeness validation.

**Key Features:**
- File integrity validation with SHA-256 checksums
- Completeness and quality scoring
- Automated retention management
- Compression and storage optimization tracking

**Usage Example:**
```python
from airflow.dags.utils.db_manager import DatabaseManager

db = DatabaseManager()

# Create backup record
backup_id = db.create_backup_record(
    file_path='/backups/market_data_20240120.csv.gz',
    data_start_date=datetime(2024, 1, 20, 8, 0),
    data_end_date=datetime(2024, 1, 20, 13, 0),
    record_count=1500,
    file_size_bytes=2048576,
    pipeline_run_id='run_20240120_080000'
)

# Update validation status
db.update_backup_validation(
    backup_id=backup_id,
    validation_status='validated',
    completeness_pct=98.5,
    quality_score=95.0
)
```

#### 2. `ready_signals`
Coordinates L0â†’WebSocket handover with priority-based processing.

**Key Features:**
- Priority-based signal processing
- Automatic expiration handling
- Acknowledgment and completion tracking
- Dependency management

**Usage Example:**
```python
# Create ready signal
signal_id = db.create_ready_signal(
    pipeline_run_id='run_20240120_080000',
    signal_type='data_ready',
    data_start_time=datetime(2024, 1, 20, 8, 0),
    data_end_time=datetime(2024, 1, 20, 8, 5),
    records_available=60,
    completeness_pct=100.0,
    priority=1
)

# Process signals
pending_signals = db.get_pending_ready_signals(signal_type='data_ready')
for signal in pending_signals.itertuples():
    db.acknowledge_ready_signal(signal.signal_id, 'websocket_service')
    # ... process data ...
    db.complete_ready_signal(signal.signal_id, 'websocket_service', 250)
```

#### 3. `data_gaps`
Tracks detected gaps and their fill status with intelligent classification.

**Key Features:**
- Automated gap detection with trading hours awareness
- Severity and impact scoring
- Fill attempt tracking with retry logic
- Resolution status management

**Usage Example:**
```python
# Detect gaps
gaps = db.detect_data_gaps(
    start_date=datetime(2024, 1, 20, 8, 0),
    end_date=datetime(2024, 1, 20, 13, 0),
    symbol='USDCOP'
)

# Create gap records
for gap in gaps:
    gap_id = db.create_data_gap_record(
        gap_start=gap['gap_start'],
        gap_end=gap['gap_end'],
        missing_points=gap['missing_points'],
        detected_by='pipeline_l0',
        detection_method='missing_sequence',
        severity='high'
    )

# Update fill status
db.update_gap_fill_status(
    gap_id=gap_id,
    fill_status='filled',
    fill_method='api_backfill',
    filled_points=58,
    fill_quality='good'
)
```

#### 4. `pipeline_health`
Monitors overall pipeline health and performance metrics.

**Key Features:**
- Component-level health scoring
- Resource utilization tracking
- Alert level management
- Performance trend analysis

**Usage Example:**
```python
# Record health metrics
health_id = db.record_pipeline_health(
    pipeline_name='l0_data_ingestion',
    component='api_connector',
    status='healthy',
    health_score=95.0,
    response_time_ms=150,
    error_rate=0.5,
    cpu_usage_pct=25.0,
    memory_usage_pct=40.0
)

# Get health summary
health_summary = db.get_pipeline_health_summary(hours_back=24)
```

## ðŸš€ Time-Series Optimizations

### New Indexes
- **BRIN indexes** for efficient time-range queries on large datasets
- **Partial indexes** for recent trading data and specific query patterns
- **GIN indexes** for JSONB metadata searches
- **Composite indexes** for multi-column queries

### Materialized Views
1. **`daily_ohlc_summary`** - Daily trading summaries with completeness metrics
2. **`hourly_trading_metrics`** - Hourly trading activity analysis
3. **`api_usage_efficiency`** - API performance and usage analytics

### Performance Functions
```sql
-- Get trading session statistics
SELECT * FROM get_trading_session_stats('2024-01-20');

-- Check real-time data quality
SELECT * FROM check_realtime_data_quality(30, 'USDCOP');

-- Refresh materialized views
SELECT refresh_trading_views();
```

## ðŸ”§ Enhanced DatabaseManager Features

### Backup Management
```python
# Comprehensive backup workflow
backup_id = db.create_backup_record(...)
db.update_backup_validation(backup_id, 'validated', 98.5, 95.0)
expired_backups = db.cleanup_expired_backups()
```

### Ready Signal Coordination
```python
# L0 Pipeline creates signal
signal_id = db.create_ready_signal(...)

# WebSocket service processes
db.acknowledge_ready_signal(signal_id, 'websocket_service')
db.complete_ready_signal(signal_id, 'websocket_service', processing_time_ms)
```

### Gap Management
```python
# Detect and track gaps
gaps = db.detect_data_gaps(start_date, end_date)
gap_id = db.create_data_gap_record(...)
db.update_gap_fill_status(gap_id, 'filled', 'api_backfill', 58)
```

### Health Monitoring
```python
# Record and monitor health
health_id = db.record_pipeline_health(...)
health_summary = db.get_pipeline_health_summary()
```

## ðŸ“Š Data Validation & Integrity

### Automatic Validations
- **OHLCV Data Validation**: Ensures High â‰¥ Low, prices within range
- **Trading Hours Detection**: Automatic classification using Colombia timezone
- **API Limit Monitoring**: Rate limiting and usage tracking
- **Data Quality Scoring**: Completeness and accuracy metrics

### Triggers & Functions
- **Auto-update timestamps** for all major tables
- **Validation triggers** for market data integrity
- **Cleanup triggers** for expired sessions and old data
- **Logging triggers** for important state changes

## ðŸ§¹ Data Retention Policies

### Automatic Cleanup Policies
```python
# Enhanced cleanup with detailed reporting
cleanup_results = db.cleanup_old_data()
# Returns: {
#   'api_usage': 1250,
#   'pipeline_runs': 45,
#   'backup_metadata': 12,
#   'ready_signals': 89,
#   'data_gaps': 5
# }
```

### Retention Schedule
- **market_data**: Permanent retention
- **trading_signals**: Permanent retention
- **pipeline_runs**: 90 days
- **api_usage**: 30 days
- **backup_metadata**: Active + 7 days after deactivation
- **ready_signals**: 7 days after processing
- **data_gaps**: 30 days after resolution
- **pipeline_health**: 30 days
- **system_metrics**: 7-90 days (varies by category)

## ðŸ” Monitoring & Analytics

### Database Statistics
```python
# Comprehensive database analytics
stats = db.get_database_statistics()
# Returns table sizes, record counts, recent activity
```

### Quality Monitoring
```python
# Data quality summary
quality_summary = db.get_data_quality_summary(run_id='...')
# Returns pass rates, metrics, trends
```

### Performance Analysis
```sql
-- Analyze table performance
SELECT * FROM analyze_table_performance();

-- Get data completeness
SELECT * FROM calculate_data_completeness('2024-01-20', '2024-01-21');
```

## ðŸš€ Setup Instructions

### 1. Database Schema Setup
Run the SQL scripts in order:
```bash
psql -d usdcop_trading -f postgres/init/01_create_tables.sql
psql -d usdcop_trading -f postgres/init/02_create_indexes.sql
psql -d usdcop_trading -f postgres/init/03_create_functions.sql
psql -d usdcop_trading -f postgres/init/04_create_triggers.sql
psql -d usdcop_trading -f postgres/init/05_insert_default_data.sql
psql -d usdcop_trading -f postgres/init/06_create_additional_tables.sql
psql -d usdcop_trading -f postgres/init/07_time_series_optimizations.sql
```

### 2. Python Integration
```python
from airflow.dags.utils.db_manager import DatabaseManager

# Initialize with default connection
db = DatabaseManager()

# Or with custom connection
db = DatabaseManager({
    'host': 'localhost',
    'port': '5432',
    'database': 'usdcop_trading',
    'user': 'admin',
    'password': 'password'
})
```

### 3. Testing
```bash
# Run comprehensive tests
cd /home/GlobalForex/USDCOP-RL-Models
python -m airflow.dags.utils.db_manager

# Test enhanced features
python -c "from airflow.dags.utils.db_manager import test_enhanced_features; test_enhanced_features()"
```

## ðŸ“ˆ Performance Benefits

### Query Performance
- **50-80% faster** time-range queries with BRIN indexes
- **Instant access** to daily/hourly summaries via materialized views
- **Reduced I/O** with optimized composite indexes

### Operational Efficiency
- **Automated gap detection** and fill tracking
- **Intelligent backup management** with integrity validation
- **Real-time health monitoring** with alert capabilities
- **Comprehensive data retention** with automated cleanup

### Scalability
- **Partitioning-ready** schema for large datasets
- **Efficient indexing** for millions of records
- **Resource monitoring** and optimization recommendations

## ðŸ”§ Maintenance

### Regular Tasks
```bash
# Refresh materialized views (run hourly during trading)
psql -d usdcop_trading -c "SELECT refresh_trading_views();"

# Daily cleanup (run at 2 AM)
python -c "from airflow.dags.utils.db_manager import DatabaseManager; db = DatabaseManager(); db.cleanup_old_data(); db.close()"

# Weekly health check
python -c "from airflow.dags.utils.db_manager import DatabaseManager; db = DatabaseManager(); print(db.get_database_statistics()); db.close()"
```

### Monitoring Queries
```sql
-- Check recent gaps
SELECT * FROM data_gaps WHERE fill_status = 'detected' ORDER BY gap_start DESC LIMIT 10;

-- Review pending signals
SELECT * FROM ready_signals WHERE status = 'pending' ORDER BY priority, created_at;

-- Monitor pipeline health
SELECT * FROM pipeline_health WHERE measured_at >= NOW() - INTERVAL '1 hour';
```

## ðŸŽ¯ Best Practices

1. **Always use transactions** for multi-table operations
2. **Monitor gap detection** during trading hours
3. **Regular backup validation** for data integrity
4. **Health check integration** in pipeline monitoring
5. **Retention policy compliance** for storage optimization

This enhanced database foundation provides robust support for the L0 pipeline system with comprehensive monitoring, backup management, and performance optimization capabilities.