# ‚úÖ FASE 0: PIPELINE L0 MACRO DATA - COMPLETADA

**Fecha:** 2025-11-05
**Status:** Archivos creados, listo para implementaci√≥n
**Duraci√≥n creaci√≥n:** ~1 hora

---

## üì¶ ARCHIVOS CREADOS (5 archivos)

### **1. scripts/verify_twelvedata_macro.py**
- **Prop√≥sito:** Verificar si WTI (CL) y DXY est√°n disponibles en TwelveData API
- **Uso:** `python scripts/verify_twelvedata_macro.py`
- **Requiere:** Variable `TWELVEDATA_API_KEY_G1` configurada

### **2. init-scripts/02-macro-data-schema.sql**
- **Prop√≥sito:** Crear tabla PostgreSQL `macro_ohlcv` (TimescaleDB hypertable)
- **Uso:** `psql -U usdcop -d usdcop_db -f init-scripts/02-macro-data-schema.sql`
- **Features:**
  - Primary key: (time, symbol)
  - Constraints OHLC validation
  - 2 funciones auxiliares: `get_macro_stats()`, `detect_macro_gaps()`

### **3. airflow/dags/usdcop_m5__01b_l0_macro_acquire.py**
- **Prop√≥sito:** DAG Airflow para descargar datos macro diariamente
- **S√≠mbolos:** WTI (CL), DXY
- **Intervalo:** 1 hora
- **Schedule:** @daily (autom√°tico)
- **Catchup:** True (descarga hist√≥ricos desde 2002)
- **Tasks:**
  1. `fetch_macro_data` - Descargar de TwelveData
  2. `insert_to_postgresql` - Insertar en `macro_ohlcv`
  3. `export_to_minio` - Exportar a bucket `00-raw-macro-marketdata`
  4. `validate_data_quality` - Validar calidad

### **4. scripts/upload_macro_manual.py**
- **Prop√≥sito:** Fallback manual si TwelveData no disponible
- **Uso:**
  ```bash
  python scripts/upload_macro_manual.py --file WTI_Historical_Data.csv --symbol WTI
  python scripts/upload_macro_manual.py --file DXY_Historical_Data.csv --symbol DXY
  ```
- **Fuente datos:** investing.com (CSV download)
- **Features:**
  - Parsea CSV de investing.com
  - Upsert a PostgreSQL
  - Upload a MinIO

### **5. FASE_0_INSTRUCCIONES.md**
- **Prop√≥sito:** Gu√≠a completa paso a paso para ejecutar Fase 0
- **Contenido:**
  - Checklist de ejecuci√≥n
  - Troubleshooting
  - M√©tricas de √©xito
  - Mantenimiento diario

---

## üöÄ PR√ìXIMOS PASOS PARA EL USUARIO

### **PASO 1: Configurar API Key (‚ö†Ô∏è REQUERIDO)**

```bash
# Opci√≥n A: Agregar a .env o docker-compose.yml
TWELVEDATA_API_KEY_G1=tu_api_key_aqui
TWELVEDATA_API_KEY_G2=otra_api_key  # Opcional (fallback)
TWELVEDATA_API_KEY_G3=otra_api_key  # Opcional (fallback)

# Opci√≥n B: Export temporal (session actual)
export TWELVEDATA_API_KEY_G1="tu_api_key_aqui"
```

**Obtener API key gratuita:**
- https://twelvedata.com/pricing
- Plan gratuito: 800 requests/d√≠a (suficiente para 2 s√≠mbolos √ó 1h data)

---

### **PASO 2: Crear Tabla PostgreSQL**

```bash
# Opci√≥n A: Desde host
docker exec -it usdcop-postgres psql -U usdcop -d usdcop_db \
  -f /init-scripts/02-macro-data-schema.sql

# Opci√≥n B: Desde container
docker exec -it usdcop-postgres bash
psql -U usdcop -d usdcop_db -f /init-scripts/02-macro-data-schema.sql
exit
```

**Verificar:**
```bash
docker exec -it usdcop-postgres psql -U usdcop -d usdcop_db \
  -c "\d macro_ohlcv"
```

---

### **PASO 3: Verificar TwelveData**

```bash
python scripts/verify_twelvedata_macro.py
```

**Si funciona (exit code 0):**
‚Üí Continuar con PASO 4A (TwelveData)

**Si falla (exit code 1):**
‚Üí Continuar con PASO 4B (Fallback Manual)

---

### **PASO 4A: Usar TwelveData API (Recomendado)**

**4A.1: Copiar DAG a Airflow**

El DAG ya est√° en la ubicaci√≥n correcta:
```
airflow/dags/usdcop_m5__01b_l0_macro_acquire.py
```

**4A.2: Reiniciar Airflow (para detectar nuevo DAG)**

```bash
docker-compose restart airflow-webserver airflow-scheduler
```

**4A.3: Activar DAG**

```bash
# Desde Airflow UI (http://localhost:8080)
# ‚Üí Buscar: usdcop_m5__01b_l0_macro_acquire
# ‚Üí Activar toggle

# O desde CLI:
docker exec -it usdcop-airflow-webserver \
  airflow dags unpause usdcop_m5__01b_l0_macro_acquire
```

**4A.4: Trigger Manual (Testing)**

```bash
docker exec -it usdcop-airflow-webserver \
  airflow dags trigger usdcop_m5__01b_l0_macro_acquire
```

**4A.5: Verificar Ejecuci√≥n**

```bash
# Ver logs
docker exec -it usdcop-airflow-webserver \
  airflow tasks logs usdcop_m5__01b_l0_macro_acquire fetch_macro_data <fecha>

# Verificar datos en PostgreSQL
docker exec -it usdcop-postgres psql -U usdcop -d usdcop_db \
  -c "SELECT * FROM get_macro_stats();"
```

**4A.6: Ejecutar Catchup Hist√≥rico**

El DAG tiene `catchup=True`, por lo que **autom√°ticamente** descargar√° todos los datos desde 2002-01-01 hasta hoy.

‚ö†Ô∏è **ADVERTENCIA:** Esto puede tardar 2-3 horas y consumir ~800 API calls/d√≠a (necesitas API key con suficiente quota).

**Monitorear progreso:**
- Airflow UI ‚Üí DAG ‚Üí Calendar View
- Deber√≠a mostrar ejecuciones diarias desde 2002

**Meta:** ~45,000 registros por s√≠mbolo (23 a√±os √ó 365 d√≠as √ó 24 horas √ó 1 registro/hora / 5 = ~40k)

---

### **PASO 4B: Fallback Manual (Si TwelveData falla)**

**4B.1: Descargar datos de investing.com**

**WTI:**
1. https://www.investing.com/commodities/crude-oil-historical-data
2. Date Range: Jan 02, 2002 ‚Üí Today
3. Download CSV ‚Üí Guardar como `WTI_Historical_Data.csv`

**DXY:**
1. https://www.investing.com/indices/usdollar-historical-data
2. Date Range: Jan 02, 2002 ‚Üí Today
3. Download CSV ‚Üí Guardar como `DXY_Historical_Data.csv`

**4B.2: Cargar con script**

```bash
python scripts/upload_macro_manual.py \
  --file ~/Downloads/WTI_Historical_Data.csv \
  --symbol WTI

python scripts/upload_macro_manual.py \
  --file ~/Downloads/DXY_Historical_Data.csv \
  --symbol DXY
```

**Meta:** ~6,000 registros por s√≠mbolo (23 a√±os √ó 252 trading days)

‚ö†Ô∏è **NOTA:** Datos manuales son **diarios**, no horarios. Se expandir√°n a 5min en L3 con forward-fill.

---

### **PASO 5: Validar Datos**

```bash
# Estad√≠sticas generales
docker exec -it usdcop-postgres psql -U usdcop -d usdcop_db \
  -c "SELECT * FROM get_macro_stats();"

# Output esperado (TwelveData):
#  symbol | record_count | min_time            | max_time            | days_coverage | source
# --------+--------------+---------------------+---------------------+---------------+-----------
#  WTI    |        45000 | 2002-01-02 00:00:00 | 2025-11-05 23:00:00 |          8674 | twelvedata
#  DXY    |        45000 | 2002-01-02 00:00:00 | 2025-11-05 23:00:00 |          8674 | twelvedata

# Output esperado (Manual):
#  symbol | record_count | min_time            | max_time            | days_coverage | source
# --------+--------------+---------------------+---------------------+---------------+--------------------
#  WTI    |         5843 | 2002-01-02 00:00:00 | 2025-11-05 00:00:00 |          8674 | investing.com_manual
#  DXY    |         5843 | 2002-01-02 00:00:00 | 2025-11-05 00:00:00 |          8674 | investing.com_manual
```

**Verificar MinIO:**
```bash
mc ls minio/00-raw-macro-marketdata/WTI/
mc ls minio/00-raw-macro-marketdata/DXY/
```

---

## ‚úÖ CRITERIOS DE √âXITO FASE 0

| M√©trica | Target TwelveData | Target Manual | Status |
|---------|------------------|---------------|--------|
| WTI registros | > 40,000 | > 5,000 | ‚¨ú |
| DXY registros | > 40,000 | > 5,000 | ‚¨ú |
| Calidad OHLC | 0% NaN | 0% NaN | ‚¨ú |
| Cobertura | 2002-2025 | 2002-2025 | ‚¨ú |
| PostgreSQL | Tabla creada | Tabla creada | ‚¨ú |
| MinIO | Archivos presentes | Archivos presentes | ‚¨ú |

**Una vez todos ‚úÖ:** Fase 0 completada ‚Üí Continuar con Fase 2

---

## üìä RESUMEN DE IMPLEMENTACI√ìN

### **Lo que hemos creado:**

```
FASE 0: Pipeline L0 Macro Data
‚îÇ
‚îú‚îÄ‚îÄ üìÑ SQL Schema (macro_ohlcv table)
‚îÇ   ‚îî‚îÄ‚îÄ TimescaleDB hypertable con constraints OHLC
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ DAG Airflow (L0 macro acquisition)
‚îÇ   ‚îú‚îÄ‚îÄ Descarga diaria autom√°tica (TwelveData API)
‚îÇ   ‚îú‚îÄ‚îÄ Catchup hist√≥rico (2002-2025)
‚îÇ   ‚îú‚îÄ‚îÄ Insert a PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ Export a MinIO
‚îÇ   ‚îî‚îÄ‚îÄ Validaci√≥n de calidad
‚îÇ
‚îú‚îÄ‚îÄ üîç Script Verificaci√≥n (verify_twelvedata_macro.py)
‚îÇ   ‚îî‚îÄ‚îÄ Prueba disponibilidad de WTI y DXY en API
‚îÇ
‚îú‚îÄ‚îÄ üì§ Script Fallback Manual (upload_macro_manual.py)
‚îÇ   ‚îú‚îÄ‚îÄ Parsea CSV de investing.com
‚îÇ   ‚îú‚îÄ‚îÄ Upload a PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ Upload a MinIO
‚îÇ
‚îî‚îÄ‚îÄ üìñ Instrucciones Completas (FASE_0_INSTRUCCIONES.md)
    ‚îî‚îÄ‚îÄ Gu√≠a paso a paso con troubleshooting
```

### **Datos que se obtendr√°n:**

**WTI Crude Oil (s√≠mbolo: CL)**
- Intervalo: 1 hora (TwelveData) o diario (manual)
- Rango: 2002-01-02 hasta hoy
- Registros esperados: ~45,000 (TwelveData) o ~6,000 (manual)
- Uso: Feature correlaci√≥n USD/COP con commodities

**US Dollar Index (s√≠mbolo: DXY)**
- Intervalo: 1 hora (TwelveData) o diario (manual)
- Rango: 2002-01-02 hasta hoy
- Registros esperados: ~45,000 (TwelveData) o ~6,000 (manual)
- Uso: Feature fuerza del d√≥lar vs otras monedas

### **D√≥nde se almacenan:**

1. **PostgreSQL:** Tabla `macro_ohlcv`
   - Query r√°pido para L3 feature engineering
   - Funciones auxiliares para estad√≠sticas y gap detection

2. **MinIO:** Bucket `00-raw-macro-marketdata/`
   - Backup en parquet comprimido
   - Archivado por s√≠mbolo y fecha

---

## ‚û°Ô∏è DESPU√âS DE FASE 0

**Orden de implementaci√≥n:**

1. ‚úÖ **FASE 0 COMPLETADA** (este documento)

2. **FASE 2: L3/L4 Feature Engineering** (siguiente)
   - Leer: `PLAN_ESTRATEGICO_v2_UPDATES.md` Secci√≥n 2
   - Modificar: `airflow/dags/usdcop_m5__04_l3_feature.py`
     - A√±adir `fetch_macro_data()`
     - A√±adir `calculate_macro_features()` (7 features)
     - A√±adir `calculate_mtf_features()` (8 features)
   - Modificar: `airflow/dags/usdcop_m5__05_l4_rlready.py`
     - Expandir OBS_MAPPING de 17 ‚Üí 45

3. **FASE 3: Reward Shaping + SAC**
   - Crear: `notebooks/utils/rewards.py`
   - Modificar: `notebooks/utils/environments.py`
   - A/B testing de reward functions

4. **FASE 4: Optuna Optimization**

5. **FASE 5: Walk-Forward Validation**

---

## üéì DOCUMENTACI√ìN DE REFERENCIA

### **Archivos clave para leer:**

```
1. FASE_0_INSTRUCCIONES.md              [Esta fase - paso a paso]
2. PLAN_ESTRATEGICO_v2_UPDATES.md       [Todas las fases con gaps integrados]
3. RESUMEN_EJECUTIVO_v2.md              [Overview completo del proyecto]
4. ADDENDUM_MACRO_FEATURES.md           [Detalles t√©cnicos macro pipeline]
5. ADDENDUM_REWARD_SHAPING.md           [Reward functions - Fase 3]
6. ADDENDUM_MTF_SPECIFICATION.md        [Multi-timeframe features - Fase 2]
```

### **Papers acad√©micos citados:**

1. **Moody & Saffell (2001)**: Differential Sharpe Ratio
2. **ICASSP (2019)**: Price Trailing Reward
3. **ArXiv (2022)**: Multi-Objective Reward
4. **Elder (2014)**: Triple Screen Method
5. **L√≥pez de Prado (2018)**: Walk-Forward con Embargo

---

## ‚ö†Ô∏è NOTAS IMPORTANTES

### **1. API Keys TwelveData**

- **Gratis:** 800 requests/d√≠a (suficiente para 2 s√≠mbolos diarios)
- **Necesario para:** Datos horarios automatizados
- **Alternativa:** Fallback manual (datos diarios de investing.com)

### **2. Catchup Hist√≥rico**

- **Tiempo:** 2-3 horas
- **Requests:** ~8,000 (23 a√±os √ó 365 d√≠as)
- **Soluci√≥n:** Ejecutar de noche o usar plan pagado de TwelveData

### **3. Datos Diarios vs Horarios**

- **TwelveData:** Datos horarios ‚Üí Mejor para features
- **Manual:** Datos diarios ‚Üí Se expanden en L3 con forward-fill
- **Ambos son v√°lidos**, pero horarios son preferibles

### **4. Mantenimiento Diario**

- **Con TwelveData:** Autom√°tico (DAG diario)
- **Con Manual:** Requiere descarga diaria de investing.com
  - Automatizar con cron o scheduled task

---

## üêõ TROUBLESHOOTING COM√öN

### **"UnicodeEncodeError" al ejecutar verify script**

**Causa:** Emojis no soportados en Windows cmd con cp1252

**Soluci√≥n:**
```bash
# Opci√≥n 1: Usar PowerShell (mejor encoding)
powershell
python scripts/verify_twelvedata_macro.py

# Opci√≥n 2: Usar Python UTF-8 mode
set PYTHONIOENCODING=utf-8
python scripts/verify_twelvedata_macro.py

# Opci√≥n 3: Comentar emojis en el script (l√≠neas 20-27)
```

### **"API key no encontrada"**

**Soluci√≥n:**
```bash
# Verificar variable configurada
echo %TWELVEDATA_API_KEY_G1%  # Windows
echo $TWELVEDATA_API_KEY_G1   # Linux/Mac

# Si vac√≠a, configurar:
set TWELVEDATA_API_KEY_G1=tu_key_aqui     # Windows cmd
$env:TWELVEDATA_API_KEY_G1="tu_key_aqui"  # PowerShell
export TWELVEDATA_API_KEY_G1="tu_key_aqui"  # Linux/Mac
```

### **"Tabla macro_ohlcv no existe"**

**Causa:** Schema SQL no ejecutado

**Soluci√≥n:** Ver PASO 2 arriba

### **"MinIO bucket not accessible"**

**Soluci√≥n:**
```bash
# Verificar MinIO corriendo
docker ps | grep minio

# Crear bucket manualmente
mc mb minio/00-raw-macro-marketdata
```

---

## ‚úÖ CHECKLIST FINAL

Antes de continuar a Fase 2, verificar:

- [ ] Tabla `macro_ohlcv` creada en PostgreSQL
- [ ] API key TwelveData configurada (o preparado fallback manual)
- [ ] Script `verify_twelvedata_macro.py` ejecutado exitosamente
- [ ] DAG `usdcop_m5__01b_l0_macro_acquire` visible en Airflow UI
- [ ] Datos macro descargados (TwelveData o manual)
- [ ] `get_macro_stats()` muestra registros > 0
- [ ] Bucket MinIO `00-raw-macro-marketdata` tiene archivos
- [ ] Le√≠do `FASE_0_INSTRUCCIONES.md` completamente

**Todos ‚úÖ ‚Üí Proceder a Fase 2**

---

**FIN DEL DOCUMENTO**

*Fase 0 completada - 2025-11-05*
*Pr√≥ximo: Fase 2 (L3/L4 Feature Engineering)*
