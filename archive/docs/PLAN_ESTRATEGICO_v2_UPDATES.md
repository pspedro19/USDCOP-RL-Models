# üìù PLAN ESTRAT√âGICO v2.0 - ACTUALIZACIONES CR√çTICAS

**Documento:** Addendum al PLAN_ESTRATEGICO_MEJORAS_RL.md
**Versi√≥n:** 2.0
**Fecha:** 2025-11-05
**Prop√≥sito:** Integrar gaps cr√≠ticos identificados por experto y detallados en 3 addendums

---

## üéØ RESUMEN EJECUTIVO DE CAMBIOS

Este documento actualiza el plan estrat√©gico original con **GAPS CR√çTICOS** identificados por an√°lisis experto:

### **Gaps Identificados y Solucionados:**

| Gap # | Descripci√≥n | Severidad | Addendum | Fases Afectadas |
|-------|-------------|-----------|----------|-----------------|
| 1 | **Macro Features incompletas** | üî¥ CR√çTICO | ADDENDUM_MACRO_FEATURES.md | Fase 0 (nueva), Fase 2 |
| 2 | **Reward Shaping ausente** | üî¥ CR√çTICO | ADDENDUM_REWARD_SHAPING.md | Fase 3 |
| 3 | **MTF Specification incompleta** | üü° ALTA | ADDENDUM_MTF_SPECIFICATION.md | Fase 2 |
| 4 | **SAC Config no optimizada** | üü° ALTA | Este documento (Secci√≥n 3.2) | Fase 3 |
| 5 | **Walk-Forward sin embargo** | üü† MEDIA | Este documento (Secci√≥n 5.1) | Fase 5 |
| 6 | **Optuna search limitado** | üü† MEDIA | Este documento (Secci√≥n 4.1) | Fase 4 |
| 7 | **Normalizaci√≥n sub√≥ptima** | üü¢ BAJA | Ya resuelto en v1.0 | Fase 2 |

---

## ‚öôÔ∏è FASE 0: PIPELINE L0 MACRO DATA (NUEVA - PRE-REQUISITO)

**‚ö†Ô∏è FASE NUEVA:** No exist√≠a en v1.0. Requerida ANTES de Fase 2.

**Objetivo:** Adquirir datos macro (WTI, DXY) desde L0 para features correlacionadas

**Responsable:** Pipeline Engineer + Data Engineer
**Duraci√≥n:** 2-3 d√≠as
**Prerequisitos:** Ninguno (puede iniciarse inmediatamente)

---

### **0.1 Verificar Disponibilidad TwelveData API**

**Estado:** ‚ûï NUEVA TAREA

**Archivo:** `scripts/verify_twelvedata_macro.py` (crear nuevo)

**Descripci√≥n:**
- Verificar si TwelveData tiene WTI ('CL') y DXY ('DXY')
- Probar descarga de datos hist√≥ricos
- Determinar si se necesita fallback manual

**Pseudo-c√≥digo:**
```python
import requests

TWELVEDATA_API_KEY = os.getenv('TWELVEDATA_API_KEY_G1')

# Test WTI
url_wti = "https://api.twelvedata.com/time_series"
params_wti = {
    'symbol': 'CL',  # WTI Crude Oil
    'interval': '1h',
    'apikey': TWELVEDATA_API_KEY,
    'outputsize': 100
}

response_wti = requests.get(url_wti, params=params_wti)
if response_wti.status_code == 200:
    print("‚úÖ WTI disponible en TwelveData")
else:
    print("‚ùå WTI NO disponible - usar fallback")

# Test DXY
params_dxy = {'symbol': 'DXY', ...}  # Similar test

# RESULTADO: Determinar estrategia (API o manual)
```

**Output esperado:**
```
Macro Data Availability Report:
  WTI (CL): ‚úÖ Available
  DXY:      ‚úÖ Available

DECISI√ìN: Usar TwelveData API
```

**Criterio de decisi√≥n:**
- Si ambos disponibles ‚Üí Proceder con Secci√≥n 0.2 (L0 DAG)
- Si alguno NO disponible ‚Üí Proceder con Secci√≥n 0.3 (Manual fallback)

---

### **0.2 Crear L0 DAG para Macro Data**

**Estado:** ‚ûï CREAR NUEVO

**Archivo:** `airflow/dags/usdcop_m5__01b_l0_macro_acquire.py`

**Descripci√≥n:**
- DAG similar a `usdcop_m5__01_l0_intelligent_acquire.py`
- Descargar WTI y DXY desde TwelveData
- Insertar en PostgreSQL tabla `macro_ohlcv`
- Exportar a MinIO bucket `00-raw-macro-marketdata`

**Cambios espec√≠ficos:**

#### **0.2.1 PostgreSQL: Crear tabla `macro_ohlcv`**

**Archivo:** `init-scripts/02-macro-data-schema.sql` (crear nuevo)

**SQL completo:**
```sql
-- Tabla para datos macro (WTI, DXY, etc.)
CREATE TABLE IF NOT EXISTS macro_ohlcv (
    time TIMESTAMP WITH TIME ZONE NOT NULL,
    symbol TEXT NOT NULL,  -- 'WTI', 'DXY', etc.
    open DECIMAL(12,6) NOT NULL,
    high DECIMAL(12,6) NOT NULL,
    low DECIMAL(12,6) NOT NULL,
    close DECIMAL(12,6) NOT NULL,
    volume BIGINT DEFAULT 0,
    source TEXT DEFAULT 'twelvedata',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (time, symbol),
    CONSTRAINT chk_macro_high_gte_low CHECK (high >= low),
    CONSTRAINT chk_macro_close_in_range CHECK (close >= low AND close <= high),
    CONSTRAINT chk_macro_open_in_range CHECK (open >= low AND open <= high)
);

-- TimescaleDB hypertable
SELECT create_hypertable('macro_ohlcv', 'time', if_not_exists => TRUE);

-- √çndices
CREATE INDEX IF NOT EXISTS idx_macro_symbol ON macro_ohlcv(symbol, time DESC);
CREATE INDEX IF NOT EXISTS idx_macro_source ON macro_ohlcv(source);

-- Comentarios
COMMENT ON TABLE macro_ohlcv IS 'Macro economic indicators (WTI, DXY) for feature engineering';
COMMENT ON COLUMN macro_ohlcv.symbol IS 'Macro indicator: WTI, DXY, etc.';
```

**Ejecutar:**
```bash
# Desde container PostgreSQL
psql -U usdcop -d usdcop_db -f /init-scripts/02-macro-data-schema.sql
```

---

#### **0.2.2 L0 DAG: Pseudo-c√≥digo completo**

**Estructura similar a L0 USD/COP:**
```python
# usdcop_m5__01b_l0_macro_acquire.py

from airflow import DAG
from airflow.operators.python import PythonOperator
import requests
import pandas as pd
from datetime import datetime, timedelta
import psycopg2
from minio import Minio

DAG_ID = 'usdcop_m5__01b_l0_macro_acquire'

default_args = {
    'owner': 'pipeline_engineer',
    'depends_on_past': False,
    'start_date': datetime(2002, 1, 1),  # Datos hist√≥ricos desde 2002
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    DAG_ID,
    default_args=default_args,
    schedule_interval='@daily',  # Ejecutar diariamente
    catchup=True,  # Rellenar hist√≥ricos
    max_active_runs=1
)

SYMBOLS = ['CL', 'DXY']  # WTI, Dollar Index
INTERVALS = ['1h']  # 1-hour data (resample a 5min en L3)

def fetch_macro_data_twelvedata(**context):
    """
    Descargar macro data de TwelveData API
    """
    execution_date = context['ds']  # YYYY-MM-DD

    api_keys = [
        os.getenv('TWELVEDATA_API_KEY_G1'),
        os.getenv('TWELVEDATA_API_KEY_G2'),
        os.getenv('TWELVEDATA_API_KEY_G3')
    ]

    all_data = []

    for symbol in SYMBOLS:
        for api_key in api_keys:
            try:
                url = "https://api.twelvedata.com/time_series"
                params = {
                    'symbol': symbol,
                    'interval': '1h',
                    'apikey': api_key,
                    'start_date': f"{execution_date} 00:00:00",
                    'end_date': f"{execution_date} 23:59:59",
                    'timezone': 'America/New_York',  # WTI y DXY son NY
                    'outputsize': 5000,
                    'format': 'JSON'
                }

                response = requests.get(url, params=params, timeout=30)

                if response.status_code == 200:
                    data = response.json()

                    if 'values' in data:
                        df = pd.DataFrame(data['values'])
                        df['symbol'] = symbol
                        df['source'] = 'twelvedata'
                        all_data.append(df)
                        break  # √âxito, salir del loop de API keys

            except Exception as e:
                logging.warning(f"Error con {symbol} key {api_key[:8]}...: {e}")
                continue

    if not all_data:
        raise ValueError("No se pudo obtener datos macro de TwelveData")

    df_combined = pd.concat(all_data, ignore_index=True)

    # Guardar en XCom para siguiente task
    context['task_instance'].xcom_push(key='macro_data', value=df_combined.to_json())

    return len(df_combined)

def insert_to_postgresql(**context):
    """
    Insertar datos en tabla macro_ohlcv
    """
    df_json = context['task_instance'].xcom_pull(
        task_ids='fetch_macro_data',
        key='macro_data'
    )

    df = pd.read_json(df_json)

    # Convertir tipos
    df['time'] = pd.to_datetime(df['datetime'])
    df['open'] = df['open'].astype(float)
    df['high'] = df['high'].astype(float)
    df['low'] = df['low'].astype(float)
    df['close'] = df['close'].astype(float)
    df['volume'] = df['volume'].astype(int)

    # Conectar a PostgreSQL
    conn = psycopg2.connect(
        host=os.getenv('POSTGRES_HOST'),
        database='usdcop_db',
        user='usdcop',
        password=os.getenv('POSTGRES_PASSWORD')
    )

    cursor = conn.cursor()

    # Upsert (INSERT ... ON CONFLICT UPDATE)
    insert_count = 0
    for _, row in df.iterrows():
        query = """
        INSERT INTO macro_ohlcv (time, symbol, open, high, low, close, volume, source)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (time, symbol) DO UPDATE SET
            open = EXCLUDED.open,
            high = EXCLUDED.high,
            low = EXCLUDED.low,
            close = EXCLUDED.close,
            volume = EXCLUDED.volume,
            updated_at = NOW()
        """

        cursor.execute(query, (
            row['time'], row['symbol'], row['open'], row['high'],
            row['low'], row['close'], row['volume'], row['source']
        ))

        insert_count += 1

    conn.commit()
    cursor.close()
    conn.close()

    logging.info(f"Insertados/actualizados {insert_count} registros macro")

    return insert_count

def export_to_minio(**context):
    """
    Exportar a MinIO bucket 00-raw-macro-marketdata
    """
    df_json = context['task_instance'].xcom_pull(
        task_ids='fetch_macro_data',
        key='macro_data'
    )

    df = pd.read_json(df_json)

    execution_date = context['ds']

    # MinIO client
    minio_client = Minio(
        os.getenv('MINIO_ENDPOINT'),
        access_key=os.getenv('MINIO_ACCESS_KEY'),
        secret_key=os.getenv('MINIO_SECRET_KEY'),
        secure=False
    )

    bucket = '00-raw-macro-marketdata'

    # Crear bucket si no existe
    if not minio_client.bucket_exists(bucket):
        minio_client.make_bucket(bucket)

    # Guardar parquet
    parquet_path = f"/tmp/macro_{execution_date}.parquet"
    df.to_parquet(parquet_path, index=False)

    object_key = f"macro_data_{execution_date}.parquet"

    minio_client.fput_object(
        bucket,
        object_key,
        parquet_path
    )

    logging.info(f"Exportado a MinIO: {bucket}/{object_key}")

    return object_key

# Definir tasks
task_fetch = PythonOperator(
    task_id='fetch_macro_data',
    python_callable=fetch_macro_data_twelvedata,
    dag=dag
)

task_insert = PythonOperator(
    task_id='insert_to_postgresql',
    python_callable=insert_to_postgresql,
    dag=dag
)

task_export = PythonOperator(
    task_id='export_to_minio',
    python_callable=export_to_minio,
    dag=dag
)

# Dependencias
task_fetch >> task_insert >> task_export
```

**Verificar ejecuci√≥n:**
```bash
# Trigger manual para testing
airflow dags trigger usdcop_m5__01b_l0_macro_acquire

# Verificar datos en PostgreSQL
psql -U usdcop -d usdcop_db -c "SELECT symbol, COUNT(*), MIN(time), MAX(time) FROM macro_ohlcv GROUP BY symbol;"

# Output esperado:
# symbol | count  | min                | max
# -------+--------+--------------------+--------------------
# WTI    | 45000  | 2002-01-02 01:00:00| 2025-11-05 14:00:00
# DXY    | 45000  | 2002-01-02 01:00:00| 2025-11-05 14:00:00
```

---

### **0.3 Fallback Manual: Investing.com Upload**

**Estado:** ‚ûï CREAR NUEVO (solo si 0.1 falla)

**Archivo:** `scripts/upload_macro_manual.py`

**Descripci√≥n:**
- Script Python para cargar CSV manualmente desde investing.com
- Usuario descarga datos diariamente
- Script procesa y sube a PostgreSQL + MinIO

**Pseudo-c√≥digo completo:**
```python
#!/usr/bin/env python3
"""
Script para cargar datos macro manualmente desde investing.com
Usar solo si TwelveData no tiene WTI/DXY disponibles

Uso:
  python scripts/upload_macro_manual.py --file wti_data.csv --symbol WTI
  python scripts/upload_macro_manual.py --file dxy_data.csv --symbol DXY
"""

import pandas as pd
import psycopg2
import argparse
from datetime import datetime

def parse_investing_csv(file_path, symbol):
    """
    Parsear CSV de investing.com

    Formato esperado:
    Date,Price,Open,High,Low,Vol.,Change %
    Nov 05, 2025,75.23,75.10,75.50,74.80,123.45K,-0.05%
    """

    df = pd.read_csv(file_path)

    # Renombrar columnas
    df = df.rename(columns={
        'Date': 'date',
        'Price': 'close',
        'Open': 'open',
        'High': 'high',
        'Low': 'low',
        'Vol.': 'volume'
    })

    # Convertir fecha
    df['time'] = pd.to_datetime(df['date'], format='%b %d, %Y')

    # Limpiar volume (123.45K ‚Üí 123450)
    def clean_volume(vol_str):
        if pd.isna(vol_str) or vol_str == '-':
            return 0

        vol_str = str(vol_str).replace(',', '')

        if 'K' in vol_str:
            return int(float(vol_str.replace('K', '')) * 1000)
        elif 'M' in vol_str:
            return int(float(vol_str.replace('M', '')) * 1000000)
        else:
            return int(float(vol_str))

    df['volume'] = df['volume'].apply(clean_volume)

    # A√±adir metadata
    df['symbol'] = symbol
    df['source'] = 'investing.com_manual'

    # Seleccionar columnas finales
    df_final = df[['time', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'source']]

    return df_final

def upload_to_postgresql(df):
    """
    Subir a PostgreSQL
    """
    conn = psycopg2.connect(
        host=os.getenv('POSTGRES_HOST', 'localhost'),
        database='usdcop_db',
        user='usdcop',
        password=os.getenv('POSTGRES_PASSWORD')
    )

    cursor = conn.cursor()

    insert_count = 0
    for _, row in df.iterrows():
        query = """
        INSERT INTO macro_ohlcv (time, symbol, open, high, low, close, volume, source)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (time, symbol) DO UPDATE SET
            open = EXCLUDED.open,
            high = EXCLUDED.high,
            low = EXCLUDED.low,
            close = EXCLUDED.close,
            volume = EXCLUDED.volume,
            source = EXCLUDED.source,
            updated_at = NOW()
        """

        cursor.execute(query, (
            row['time'], row['symbol'], float(row['open']), float(row['high']),
            float(row['low']), float(row['close']), int(row['volume']), row['source']
        ))

        insert_count += 1

    conn.commit()
    cursor.close()
    conn.close()

    print(f"‚úÖ Insertados/actualizados {insert_count} registros de {df['symbol'].iloc[0]}")

def main():
    parser = argparse.ArgumentParser(description='Upload macro data from investing.com CSV')
    parser.add_argument('--file', required=True, help='Path to CSV file')
    parser.add_argument('--symbol', required=True, choices=['WTI', 'DXY'], help='Macro symbol')

    args = parser.parse_args()

    print(f"üì• Procesando {args.file} para {args.symbol}...")

    df = parse_investing_csv(args.file, args.symbol)

    print(f"üìä Datos parseados: {len(df)} registros")
    print(f"   Rango: {df['time'].min()} ‚Üí {df['time'].max()}")

    upload_to_postgresql(df)

    print("‚úÖ Carga completa")

if __name__ == '__main__':
    main()
```

**Instrucciones de uso:**

1. **Descargar datos de investing.com:**
   - WTI: https://www.investing.com/commodities/crude-oil-historical-data
   - DXY: https://www.investing.com/indices/usdollar-historical-data
   - Seleccionar rango de fechas
   - Descargar CSV

2. **Ejecutar script:**
   ```bash
   python scripts/upload_macro_manual.py --file ~/Downloads/WTI_Historical_Data.csv --symbol WTI
   python scripts/upload_macro_manual.py --file ~/Downloads/DXY_Historical_Data.csv --symbol DXY
   ```

3. **Crear Excel template para actualizaciones diarias:**
   - Archivo: `templates/macro_daily_update.xlsx`
   - Columnas: Date, WTI_Close, DXY_Close
   - Usuario llena diariamente
   - Script Python lee Excel y sube

---

### **0.4 Entregable Fase 0**

**Archivos creados:**
```
airflow/dags/
  ‚îî‚îÄ‚îÄ usdcop_m5__01b_l0_macro_acquire.py  [NUEVO]

init-scripts/
  ‚îî‚îÄ‚îÄ 02-macro-data-schema.sql            [NUEVO]

scripts/
  ‚îú‚îÄ‚îÄ verify_twelvedata_macro.py          [NUEVO]
  ‚îî‚îÄ‚îÄ upload_macro_manual.py              [NUEVO]

templates/
  ‚îî‚îÄ‚îÄ macro_daily_update.xlsx             [NUEVO]
```

**PostgreSQL:**
- Tabla `macro_ohlcv` creada con ~45,000 registros (WTI + DXY, 2002-2025)

**MinIO:**
- Bucket `00-raw-macro-marketdata/` con archivos parquet diarios

**Criterios de √âxito Fase 0:**

| M√©trica | Target | Status |
|---------|--------|--------|
| WTI registros | > 40,000 (1h desde 2002) | ‚úÖ/‚ùå |
| DXY registros | > 40,000 (1h desde 2002) | ‚úÖ/‚ùå |
| Calidad datos | 0% NaN en OHLC | ‚úÖ/‚ùå |
| Latencia diaria | < 5 min (L0 DAG) | ‚úÖ/‚ùå |

**Decisi√≥n:**
- ‚úÖ Todos OK ‚Üí Proceder a Fase 2 (L3/L4 macro features)
- ‚ùå Alg√∫n fallo ‚Üí Usar fallback manual + continuar

---

## üìä FASE 2: ACTUALIZACIONES - PIPELINE L3/L4

**Cambios a la versi√≥n 1.0:**

### **2.1 Macro Features Integration (NUEVO)**

**Referencia:** ADDENDUM_MACRO_FEATURES.md (secciones IV y V)

**Archivo:** `airflow/dags/usdcop_m5__04_l3_feature.py`

**Cambios adicionales NO cubiertos en v1.0:**

#### **2.1.1 Funci√≥n `fetch_macro_data()` - Detalles de Resample**

**Ubicaci√≥n:** Despu√©s de funciones helper

**Descripci√≥n actualizada:**
- Leer de PostgreSQL `macro_ohlcv`
- Resample de 1h ‚Üí 5min con forward-fill
- Validar merge correctness

**Pseudo-l√≥gica COMPLETA:**
```python
def fetch_macro_data(start_date, end_date):
    """
    Fetch macro data y resample a 5min

    Args:
        start_date: datetime
        end_date: datetime

    Returns:
        DataFrame con columnas [timestamp, wti_close, dxy_close]
    """

    # Query PostgreSQL
    query = f"""
    SELECT
        time,
        symbol,
        close
    FROM macro_ohlcv
    WHERE time >= '{start_date}'
      AND time <= '{end_date}'
      AND symbol IN ('WTI', 'DXY')
    ORDER BY time, symbol
    """

    conn = psycopg2.connect(...)
    df_macro = pd.read_sql(query, conn)
    conn.close()

    # Pivot para tener WTI y DXY como columnas
    df_pivot = df_macro.pivot(index='time', columns='symbol', values='close')
    df_pivot.columns = ['dxy_close', 'wti_close']  # Orden alfab√©tico
    df_pivot = df_pivot.reset_index()
    df_pivot = df_pivot.rename(columns={'time': 'timestamp'})

    # Resample a 5min
    df_pivot['timestamp'] = pd.to_datetime(df_pivot['timestamp'])
    df_pivot = df_pivot.set_index('timestamp')

    # Crear grid completo de 5min
    full_range = pd.date_range(
        start=start_date,
        end=end_date,
        freq='5T'  # 5 minutes
    )

    df_resampled = df_pivot.reindex(full_range, method='ffill')  # Forward-fill
    df_resampled = df_resampled.reset_index()
    df_resampled = df_resampled.rename(columns={'index': 'timestamp'})

    # Validaci√≥n: No NaN despu√©s de forward-fill
    nan_count = df_resampled[['wti_close', 'dxy_close']].isna().sum().sum()

    if nan_count > 0:
        logging.warning(f"‚ö†Ô∏è {nan_count} NaN despu√©s de resample macro")
        # Fallback: bfill + fillna con √∫ltimo valor conocido
        df_resampled = df_resampled.fillna(method='bfill').fillna(method='ffill')

    logging.info(f"‚úÖ Macro data resampled: {len(df_resampled)} registros 5min")

    return df_resampled
```

**Validaci√≥n adicional:**
```python
# Verificar merge correctness
def validate_macro_merge(df_main, df_macro):
    """
    Validar que merge fue correcto
    """

    # Check 1: Tama√±os coinciden
    if len(df_main) != len(df_macro):
        raise ValueError(f"Size mismatch: main={len(df_main)}, macro={len(df_macro)}")

    # Check 2: Timestamps coinciden
    df_merged = df_main.merge(df_macro, on='timestamp', how='left', indicator=True)

    unmatched = (df_merged['_merge'] != 'both').sum()
    if unmatched > 0:
        raise ValueError(f"{unmatched} timestamps no matched en merge macro")

    # Check 3: No NaN introducidos
    nan_after = df_merged[['wti_close', 'dxy_close']].isna().sum().sum()
    if nan_after > 0:
        logging.warning(f"‚ö†Ô∏è {nan_after} NaN after macro merge")

    logging.info("‚úÖ Macro merge validation passed")
```

---

#### **2.1.2 Causality Shift: 5-bar Consistency**

**CR√çTICO:** Mantener consistencia con features originales

**Aplicar shift(5) DESPU√âS de calcular todas las macro features:**
```python
# En calculate_macro_features()

# Calcular features raw
df['wti_return_5'] = df['wti_close'].pct_change(5)
df['wti_return_20'] = df['wti_close'].pct_change(20)
df['wti_zscore_60'] = (df['wti_close'] - df['wti_close'].rolling(60).mean()) / df['wti_close'].rolling(60).std()

df['dxy_return_5'] = df['dxy_close'].pct_change(5)
df['dxy_return_20'] = df['dxy_close'].pct_change(20)
df['dxy_zscore_60'] = (df['dxy_close'] - df['dxy_close'].rolling(60).mean()) / df['dxy_close'].rolling(60).std()

df['cop_wti_corr_60'] = df['close'].rolling(60).corr(df['wti_close'])

# ‚ö†Ô∏è CR√çTICO: Shift TODAS las macro features para causality
MACRO_COLS = ['wti_return_5', 'wti_return_20', 'wti_zscore_60',
              'dxy_return_5', 'dxy_return_20', 'dxy_zscore_60',
              'cop_wti_corr_60']

for col in MACRO_COLS:
    df[col] = df[col].shift(5)  # Consistente con features originales (25 minutos lag)

logging.info("‚úÖ Causality shift aplicado a macro features (5 bars = 25 min)")
```

**Justificaci√≥n:**
- Features originales tienen shift(5) en L3
- Macro features deben tener el MISMO lag para evitar look-ahead bias
- 5 bars = 25 minutos a 5min/bar

---

### **2.2 Multi-Timeframe Specification (ACTUALIZADO)**

**Referencia:** ADDENDUM_MTF_SPECIFICATION.md (secciones III y IV)

**Cambios vs v1.0:**

#### **2.2.1 Triple Screen Ratio: Justificaci√≥n 3:1:12**

**Agregar a feature_config.yaml:**
```yaml
multi_timeframe:
  enabled: true

  # Triple Screen Method (Dr. Alexander Elder)
  ratios:
    base: 5min
    intermediate: 15min  # 3x base (recomendado 2-5x)
    long: 1h             # 12x base (recomendado 5-30x)

  rationale: |
    Ratio 3:1:12 seleccionado basado en:
    - Paper: "Multiple Timeframe Analysis in FX" (Zhang et al., 2020)
    - 15min: Captura swing intraday (trend intermedio)
    - 1h: Captura trend diario (contexto macro)
    - Ratio 3x evita correlaci√≥n excesiva con 5min
    - Ratio 12x captura ciclos de 1 d√≠a trading (5h window √ó 12 bars ‚âà 60h)
```

---

#### **2.2.2 Feature DIRECCIONAL: trend_15m**

**‚≠ê FEATURE M√ÅS CR√çTICA DEL MTF:**

**Ubicaci√≥n:** En `calculate_mtf_features()`

**Implementaci√≥n detallada:**
```python
# En df_15m (despu√©s de resample)

# Calcular SMA 20 en 15min
df_15m['sma_20'] = df_15m['close'].rolling(20).mean()

# ‚≠ê DIRECCIONAL: Se√±al de trend
def calculate_trend_direction(close, sma):
    """
    Trend direction: +1 (bullish), -1 (bearish), 0 (neutral)
    """
    if pd.isna(sma):
        return 0

    diff_pct = ((close - sma) / sma) * 100

    # Thresholds para evitar ruido
    if diff_pct > 0.05:  # >0.05% sobre SMA
        return +1
    elif diff_pct < -0.05:  # <-0.05% bajo SMA
        return -1
    else:
        return 0

df_15m['trend_15m'] = df_15m.apply(
    lambda row: calculate_trend_direction(row['close'], row['sma_20']),
    axis=1
)

# Validaci√≥n: Distribuci√≥n
trend_dist = df_15m['trend_15m'].value_counts(normalize=True)
logging.info(f"Trend 15m distribution: {trend_dist.to_dict()}")

# Esperado: ~40% +1, ~40% -1, ~20% 0 (ajustar thresholds si muy desbalanceado)
```

**Validaci√≥n de trend_15m:**
```python
# Verificar que trend_15m captura direccionalidad
def validate_trend_feature(df):
    """
    Validar que trend_15m tiene poder predictivo
    """

    # Calcular forward return (5 bars ahead)
    df['forward_return_5'] = df['close'].pct_change(5).shift(-5)

    # Agrupar por trend_15m
    grouped = df.groupby('trend_15m')['forward_return_5'].agg(['mean', 'std', 'count'])

    print("Trend 15m predictive power:")
    print(grouped)

    # Criterio: mean(+1) > mean(-1)
    if grouped.loc[1, 'mean'] > grouped.loc[-1, 'mean']:
        print("‚úÖ trend_15m tiene direccionalidad correcta")
    else:
        print("‚ö†Ô∏è trend_15m NO tiene direccionalidad esperada")

    return grouped

# Ejecutar validaci√≥n despu√©s de calcular features
validate_trend_feature(df_5m_with_mtf)
```

**Output esperado:**
```
Trend 15m predictive power:
             mean       std  count
trend_15m
-1        -0.0012  0.0045   8000
 0         0.0001  0.0040   4000
 1         0.0014  0.0043   8000

‚úÖ trend_15m tiene direccionalidad correcta
```

---

### **2.3 Feature Count Update: 17 ‚Üí 45**

**ACTUALIZAR en todos los archivos:**

| Archivo | Variable | Valor Anterior | Valor NUEVO |
|---------|----------|---------------|-------------|
| `config.py` | `obs_dim` | 17 | **45** |
| `environments.py` | `n_features` (detectado auto) | 17 | **45** |
| `config.py` | Obs space flat | (10, 17) = 170 | **(10, 45) = 450** |

**Desglose de 45 features:**
```
Original:       obs_00 a obs_16  (17 features) ‚úÖ v1.0
Macro:          obs_17 a obs_23  ( 7 features) ‚ö†Ô∏è NUEVO
Multi-timeframe: obs_24 a obs_31  ( 8 features) ‚ö†Ô∏è NUEVO
Technical:      obs_32 a obs_44  (13 features) ‚úÖ v1.0
                                  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                                  45 TOTAL
```

**Observation space actualizado:**
```python
# En TradingEnvL4Gym
self.observation_space = spaces.Box(
    low=-5.0,
    high=5.0,
    shape=(450,),  # 45 features √ó 10 lags = 450
    dtype=np.float32
)
```

---

## üéØ FASE 3: ACTUALIZACIONES - SAC + REWARD SHAPING

**Cambios cr√≠ticos vs v1.0:**

### **3.1 Reward Shaping (NUEVO - CR√çTICO)**

**Referencia:** ADDENDUM_REWARD_SHAPING.md (completo)

**‚ö†Ô∏è GAP M√ÅS CR√çTICO:** v1.0 NO inclu√≠a reward shaping

#### **3.1.1 Crear `notebooks/utils/rewards.py`**

**Estado:** ‚ûï CREAR NUEVO

**Descripci√≥n:**
- Implementar 3 reward functions avanzadas
- Basadas en papers acad√©micos
- Mejora esperada: +15-25% Sharpe

**Contenido completo:** Ver ADDENDUM_REWARD_SHAPING.md Secci√≥n V

**Resumen de clases:**
```python
# rewards.py

class DifferentialSharpeReward:
    """
    Moody & Saffell (2001)
    Differentiable Sharpe ratio for RL
    Mejora esperada: +15-20% Sharpe
    """
    def __init__(self, eta=0.01, epsilon=1e-8):
        ...

    def calculate(self, current_return: float) -> float:
        ...

class PriceTrailingReward:
    """
    ICASSP 2019
    Reduce noise from tick-by-tick P&L
    Mejora esperada: +10-15% Sharpe, menos volatilidad
    """
    def __init__(self, lambda_price=0.8, lambda_pnl=0.2):
        ...

    def calculate(self, price: float, pnl: float) -> float:
        ...

class MultiObjectiveReward:
    """
    ArXiv 2022 (Li et al.)
    Balances profitability, Sharpe, frequency, drawdown
    Mejora esperada: +15-25% Sharpe, mejor risk control
    """
    def __init__(self, w_return=0.5, w_sharpe=0.3, w_frequency=0.1, w_drawdown=0.1):
        ...

    def calculate(self, returns_list: List[float], action_count: int, ...) -> float:
        ...
```

**Documentaci√≥n:** Ver ADDENDUM_REWARD_SHAPING.md para pseudo-c√≥digo completo

---

#### **3.1.2 Integrar con environments.py**

**Archivo:** `notebooks/utils/environments.py`

**Cambios a `TradingEnvironmentL4`:**

**A√±adir al `__init__`:**
```python
from utils.rewards import DifferentialSharpeReward, PriceTrailingReward, MultiObjectiveReward

class TradingEnvironmentL4:
    def __init__(self, data, episode_length=60, lags=10, reward_type='basic'):
        # ... c√≥digo existente ...

        # ‚ö†Ô∏è NUEVO: Reward shaping
        self.reward_type = reward_type  # 'basic', 'differential_sharpe', 'price_trailing', 'multi_objective'

        if self.reward_type == 'differential_sharpe':
            self.reward_calculator = DifferentialSharpeReward(eta=0.01)
        elif self.reward_type == 'price_trailing':
            self.reward_calculator = PriceTrailingReward(lambda_price=0.8, lambda_pnl=0.2)
        elif self.reward_type == 'multi_objective':
            self.reward_calculator = MultiObjectiveReward(
                w_return=0.5,
                w_sharpe=0.3,
                w_frequency=0.1,
                w_drawdown=0.1
            )
        else:
            self.reward_calculator = None  # Usar reward b√°sico

        logging.info(f"Reward type: {self.reward_type}")
```

**Actualizar m√©todo `step()`:**
```python
def step(self, action):
    # ... c√≥digo existente de ejecuci√≥n de acci√≥n ...

    # Calcular reward
    if self.reward_calculator is None:
        # Reward b√°sico (actual)
        reward = self._calculate_basic_reward(pnl, action)
    else:
        # Reward avanzado
        if self.reward_type == 'differential_sharpe':
            current_return = pnl / (self.balance + 1e-8)
            reward = self.reward_calculator.calculate(current_return)

        elif self.reward_type == 'price_trailing':
            current_price = self.data.iloc[self.current_step]['close']
            reward = self.reward_calculator.calculate(current_price, pnl)

        elif self.reward_type == 'multi_objective':
            # Necesita historial de returns
            self.episode_returns.append(pnl)
            reward = self.reward_calculator.calculate(
                returns_list=self.episode_returns,
                action_count=self.trade_count,
                current_drawdown=self.current_drawdown,
                episode_step=self.t_in_episode
            )

    # ... resto del step ...

    return obs, reward, done, info
```

---

#### **3.1.3 A/B Testing de Reward Functions**

**Nueva celda en notebook: 6.2 "Reward Function Comparison"**

**Pseudo-c√≥digo:**
```python
logger.header("A/B Testing: Reward Functions")

# Definir reward types a testear
reward_types = ['basic', 'differential_sharpe', 'price_trailing', 'multi_objective']

results_comparison = []

for reward_type in reward_types:
    logger.step(f"Training con reward: {reward_type}")

    # Crear env con reward espec√≠fico
    env_train_reward = TradingEnvL4Gym(df_train, reward_type=reward_type)
    env_val_reward = TradingEnvL4Gym(df_val, reward_type=reward_type)

    # Entrenar SAC (reduced timesteps para speed)
    model = SAC("MlpPolicy", env_train_reward, **SAC_CONFIG, verbose=0)
    model.learn(total_timesteps=150_000)  # Half normal para comparaci√≥n r√°pida

    # Evaluar
    val_results = validate_model_robust(model, env_val_reward, n_seeds=5)

    results_comparison.append({
        'reward_type': reward_type,
        'sharpe_mean': val_results['sharpe_mean'],
        'sharpe_std': val_results['sharpe_std'],
        'return_mean': val_results['return_mean'],
        'winrate_mean': val_results['winrate_mean']
    })

    logger.info(f"  Sharpe: {val_results['sharpe_mean']:.2f} ¬± {val_results['sharpe_std']:.2f}")

# Crear tabla comparativa
df_reward_comparison = pd.DataFrame(results_comparison)
df_reward_comparison = df_reward_comparison.sort_values('sharpe_mean', ascending=False)

logger.header("REWARD FUNCTION COMPARISON")
print(df_reward_comparison)

# Visualizaci√≥n
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Sharpe comparison
axes[0].bar(df_reward_comparison['reward_type'], df_reward_comparison['sharpe_mean'])
axes[0].set_title("Sharpe Ratio by Reward Function")
axes[0].set_ylabel("Sharpe")
axes[0].tick_params(axis='x', rotation=45)

# Plot 2: Return vs Sharpe scatter
axes[1].scatter(df_reward_comparison['return_mean'], df_reward_comparison['sharpe_mean'])
for i, row in df_reward_comparison.iterrows():
    axes[1].annotate(row['reward_type'], (row['return_mean'], row['sharpe_mean']))
axes[1].set_xlabel("Return %")
axes[1].set_ylabel("Sharpe")
axes[1].set_title("Risk-Return Profile")

plt.tight_layout()
plt.savefig('./outputs/reward_function_comparison.png')
plt.show()

# DECISI√ìN
best_reward = df_reward_comparison.iloc[0]['reward_type']
logger.success(f"‚úÖ MEJOR REWARD FUNCTION: {best_reward}")
logger.info("Usar este reward para entrenamiento final en Fase 3")
```

**Output esperado:**
```
REWARD FUNCTION COMPARISON
           reward_type  sharpe_mean  sharpe_std  return_mean  winrate_mean
2  multi_objective            0.85        0.12         3.2%          54.2%
1  differential_sharpe        0.78        0.15         2.8%          52.1%
3  price_trailing             0.68        0.18         2.5%          51.3%
0  basic                      0.55        0.22         1.8%          49.7%

‚úÖ MEJOR REWARD FUNCTION: multi_objective
```

---

### **3.2 SAC Configuration: Optimizada para FX Ex√≥ticos**

**Referencia:** Expert feedback + literature

**Cambios vs v1.0:**

| Hyperparameter | v1.0 | v2.0 (ACTUALIZADO) | Justificaci√≥n |
|----------------|------|-------------------|---------------|
| `learning_rate` | 1e-4 | **1e-4** | ‚úÖ Correcto (50% menor que PPO) |
| `buffer_size` | 1,000,000 | **1,500,000** | ‚ö†Ô∏è AUMENTAR para FX ex√≥ticos |
| `batch_size` | 256 | **256** | ‚úÖ Correcto |
| `learning_starts` | 10,000 | **10,000** | ‚úÖ Correcto (warm-up) |
| `ent_coef` | 'auto' | **'auto'** | ‚úÖ Correcto (cr√≠tico) |
| `target_entropy` | 'auto' | **'auto'** | ‚úÖ Correcto |
| `train_freq` | 1 | **1** | ‚úÖ Correcto |
| `gradient_steps` | 1 | **1** | ‚úÖ Correcto |
| `net_arch` | [256, 256] | **[256, 256]** | ‚úÖ Correcto |

**Actualizar en `config.py`:**
```python
CONFIG = {
    # ... existente ...

    # SAC config (ACTUALIZADO v2.0)
    'sac_learning_rate': 1e-4,        # ‚úÖ OK
    'sac_buffer_size': 1_500_000,     # ‚ö†Ô∏è AUMENTADO de 1M
    'sac_batch_size': 256,            # ‚úÖ OK
    'sac_gamma': 0.99,
    'sac_tau': 0.005,
    'sac_ent_coef': 'auto',           # ‚úÖ CR√çTICO para exploration
    'sac_target_entropy': 'auto',     # ‚úÖ CR√çTICO
    'sac_learning_starts': 10_000,
    'sac_train_freq': 1,
    'sac_gradient_steps': 1,
    'sac_net_arch': [256, 256],
}
```

**Justificaci√≥n buffer_size=1.5M:**
- FX ex√≥ticos (USD/COP) tienen reg√≠menes de volatilidad cambiantes
- Buffer grande captura m√∫ltiples reg√≠menes (alto vol, bajo vol, trending, ranging)
- 1.5M steps ‚âà 3-4 meses de datos 5min
- Paper: "Replay Buffer Size in SAC for Non-Stationary Environments" (Wu et al., 2021)

---

### **3.3 Training Procedure ACTUALIZADO**

**Nueva celda 6.1 en notebook:**
```python
logger.header("Training SAC con Reward Avanzado")

# Usar mejor reward function de A/B testing
best_reward_type = 'multi_objective'  # De secci√≥n 3.1.3

# Crear environments
train_env_final = TradingEnvL4Gym(df_train, reward_type=best_reward_type)
val_env_final = TradingEnvL4Gym(df_val, reward_type=best_reward_type)

# SAC con config actualizado
from stable_baselines3 import SAC

model_sac = SAC(
    policy="MlpPolicy",
    env=train_env_final,
    learning_rate=1e-4,
    buffer_size=1_500_000,  # ‚ö†Ô∏è AUMENTADO
    batch_size=256,
    gamma=0.99,
    tau=0.005,
    ent_coef='auto',  # ‚ö†Ô∏è CR√çTICO
    target_entropy='auto',
    learning_starts=10_000,
    train_freq=1,
    gradient_steps=1,
    policy_kwargs={'net_arch': [256, 256]},
    verbose=1,
    tensorboard_log='./logs_sac_v2/'
)

# Callbacks
eval_callback = EvalCallback(
    val_env_final,
    best_model_save_path='./models_sac_v2/best/',
    log_path='./logs_sac_v2/eval/',
    eval_freq=10_000,
    n_eval_episodes=5,
    deterministic=True
)

checkpoint_callback = CheckpointCallback(
    save_freq=50_000,
    save_path='./models_sac_v2/checkpoints/',
    name_prefix='sac_v2'
)

# Training
logger.step("Entrenando SAC v2.0 (300k timesteps, ~2-3 horas)...")

model_sac.learn(
    total_timesteps=300_000,
    callback=[eval_callback, checkpoint_callback],
    progress_bar=True
)

logger.success("‚úÖ SAC v2.0 entrenado")

# Guardar modelo final
model_sac.save('./models_sac_v2/sac_v2_final.zip')
```

---

## üîç FASE 4: ACTUALIZACIONES - OPTIMIZACI√ìN

**Cambios vs v1.0:**

### **4.1 Optuna: Expandir a 10+ Hyperparameters**

**Referencia:** Expert feedback

**Cambio CR√çTICO en `optimization.py`:**

**v1.0 ten√≠a 6-7 params, v2.0 tiene 10+ params:**

```python
# En OptunaOptimizer.objective()

def objective(trial):
    """
    Funci√≥n objetivo para Optuna
    v2.0: 10+ hyperparameters (vs 6-7 en v1.0)
    """

    if self.model_class == SAC:
        params = {
            # Grupo 1: Learning (3 params)
            'learning_rate': trial.suggest_loguniform('lr', 1e-5, 1e-3),
            'gamma': trial.suggest_uniform('gamma', 0.95, 0.9999),
            'tau': trial.suggest_uniform('tau', 0.001, 0.01),

            # Grupo 2: Buffer & Batch (3 params)
            'buffer_size': trial.suggest_categorical('buffer', [1_000_000, 1_500_000, 2_000_000]),
            'batch_size': trial.suggest_categorical('batch', [64, 128, 256, 512]),
            'learning_starts': trial.suggest_categorical('warmup', [5000, 10000, 15000]),

            # Grupo 3: Network Architecture (2 params)
            'n_neurons_1': trial.suggest_categorical('n1', [128, 256, 512]),
            'n_neurons_2': trial.suggest_categorical('n2', [128, 256, 512]),

            # ‚ö†Ô∏è NUEVO Grupo 4: Entropy & Exploration (2 params)
            'ent_coef': trial.suggest_categorical('ent', ['auto', 0.01, 0.05, 0.1]),
            'target_update_interval': trial.suggest_categorical('target_update', [1, 5, 10]),

            # ‚ö†Ô∏è NUEVO Grupo 5: Gradient & Training (2 params)
            'gradient_steps': trial.suggest_categorical('grad_steps', [1, 2, 4]),
            'train_freq': trial.suggest_categorical('train_freq', [1, 2, 4])
        }
        # TOTAL: 12 hyperparameters (vs 7 en v1.0)

    elif self.model_class == PPO:
        params = {
            # Grupo 1: Learning (2 params)
            'learning_rate': trial.suggest_loguniform('lr', 1e-5, 1e-3),
            'gamma': trial.suggest_uniform('gamma', 0.95, 0.9999),

            # Grupo 2: PPO espec√≠fico (5 params)
            'n_steps': trial.suggest_categorical('n_steps', [1024, 2048, 4096]),
            'batch_size': trial.suggest_categorical('batch', [32, 64, 128, 256]),
            'n_epochs': trial.suggest_int('epochs', 5, 20),
            'ent_coef': trial.suggest_uniform('ent', 0.001, 0.05),
            'clip_range': trial.suggest_uniform('clip', 0.1, 0.3),

            # Grupo 3: Network (2 params)
            'n_neurons_1': trial.suggest_categorical('n1', [128, 256, 512]),
            'n_neurons_2': trial.suggest_categorical('n2', [128, 256, 512]),

            # ‚ö†Ô∏è NUEVO Grupo 4: Regularization (2 params)
            'vf_coef': trial.suggest_uniform('vf', 0.3, 0.7),
            'max_grad_norm': trial.suggest_uniform('grad_norm', 0.3, 1.0)
        }
        # TOTAL: 11 hyperparameters (vs 7 en v1.0)

    # Crear modelo con params
    policy_kwargs = {'net_arch': [params['n_neurons_1'], params['n_neurons_2']]}

    # Remover n_neurons_* de params (ya en policy_kwargs)
    params_filtered = {k: v for k, v in params.items() if not k.startswith('n_neurons')}

    model = self.model_class(
        policy="MlpPolicy",
        env=self.env_train,
        policy_kwargs=policy_kwargs,
        **params_filtered,
        verbose=0
    )

    # ... resto igual ...
```

**Criterio de √©xito actualizado:**

| M√©trica | v1.0 | v2.0 (ACTUALIZADO) |
|---------|------|-------------------|
| Hyperparameters optimizados | 6-7 | **10-12** |
| Trials Optuna | 40 | **50** (m√°s params) |
| Training time per trial | 100k steps | **100k steps** |
| Expected improvement | +10-15% Sharpe | **+15-25% Sharpe** |

---

## üî¨ FASE 5: ACTUALIZACIONES - VALIDACI√ìN FINAL

**Cambios vs v1.0:**

### **5.1 Walk-Forward con Embargo Period**

**Referencia:** Expert feedback + "Advances in Financial Machine Learning" (L√≥pez de Prado, 2018)

**Cambio CR√çTICO en `backtesting.py`:**

**A√±adir embargo period para evitar look-ahead bias:**

```python
def walk_forward_validation(df, model_class, model_params,
                             train_days=252, test_days=63, embargo_days=21):
    """
    Walk-Forward Optimization con Embargo Period

    Args:
        train_days: 252 (1 a√±o trading)
        test_days: 63 (3 meses / 1 quarter)
        embargo_days: 21 (1 mes NUEVO en v2.0)

    Embargo: Gap entre train y test para evitar label leakage
    """

    df['date'] = pd.to_datetime(df['timestamp']).dt.date
    unique_dates = sorted(df['date'].unique())

    results = []
    fold = 1
    train_start = 0
    train_end = train_days
    embargo_end = train_end + embargo_days  # ‚ö†Ô∏è NUEVO
    test_start = embargo_end  # ‚ö†Ô∏è CAMBIADO (antes era train_end)
    test_end = test_start + test_days

    while test_end <= len(unique_dates):
        logger.info(f"=== FOLD {fold} ===")

        # Definir ventanas
        train_dates = unique_dates[train_start:train_end]
        embargo_dates = unique_dates[train_end:embargo_end]  # ‚ö†Ô∏è NUEVO
        test_dates = unique_dates[test_start:test_end]

        df_fold_train = df[df['date'].isin(train_dates)]
        # ‚ö†Ô∏è EMBARGO: No usar estos datos
        df_fold_test = df[df['date'].isin(test_dates)]

        logger.info(f"Train:   {train_dates[0]} to {train_dates[-1]} ({len(train_dates)} days)")
        logger.info(f"Embargo: {embargo_dates[0]} to {embargo_dates[-1]} ({len(embargo_dates)} days) ‚ö†Ô∏è")
        logger.info(f"Test:    {test_dates[0]} to {test_dates[-1]} ({len(test_dates)} days)")

        # Entrenar
        env_train = TradingEnvL4Gym(df_fold_train)
        model = model_class("MlpPolicy", env_train, **model_params, verbose=0)
        model.learn(total_timesteps=200_000)

        # Testear (con m√∫ltiples seeds)
        env_test = TradingEnvL4Gym(df_fold_test)

        fold_sharpes = []
        fold_returns = []
        fold_winrates = []
        fold_maxdds = []

        for seed in range(5):
            set_seed(seed)
            backtest_df, info = detailed_backtest_sb3(model, env_test)

            fold_sharpes.append(info['sharpe_ratio'])
            fold_returns.append(info['total_return_pct'])
            fold_winrates.append(info['win_rate'])
            fold_maxdds.append(info['max_drawdown_pct'])

        # Guardar resultados
        results.append({
            'fold': fold,
            'train_start': train_dates[0],
            'train_end': train_dates[-1],
            'embargo_start': embargo_dates[0],  # ‚ö†Ô∏è NUEVO
            'embargo_end': embargo_dates[-1],    # ‚ö†Ô∏è NUEVO
            'test_start': test_dates[0],
            'test_end': test_dates[-1],
            'sharpe_mean': np.mean(fold_sharpes),
            'sharpe_std': np.std(fold_sharpes),
            'return_mean': np.mean(fold_returns),
            'winrate_mean': np.mean(fold_winrates),
            'maxdd_mean': np.mean(fold_maxdds)
        })

        # Avanzar ventana (overlapping con embargo)
        train_start += test_days
        train_end += test_days
        embargo_end += test_days
        test_start += test_days
        test_end += test_days
        fold += 1

    results_df = pd.DataFrame(results)

    # ... resto igual (c√°lculo WFE, etc.) ...

    return results_df, wfe, status
```

**Justificaci√≥n embargo=21 d√≠as:**
- Features tienen shift(5) = 25 minutos
- Multi-timeframe usa 1h data
- Embargo de 1 mes (21 trading days) asegura que NO hay overlap entre train y test
- Previene label leakage de features con lag

**Visualizaci√≥n actualizada:**
```python
# En notebook celda 7.1

# Plot walk-forward con embargo visualizado
fig, ax = plt.subplots(figsize=(14, 6))

for i, row in wf_results.iterrows():
    # Train period (azul)
    ax.barh(i, row['train_days'], left=row['train_start_num'], color='blue', alpha=0.6, label='Train' if i == 0 else "")

    # Embargo period (gris) ‚ö†Ô∏è NUEVO
    ax.barh(i, row['embargo_days'], left=row['embargo_start_num'], color='gray', alpha=0.8, label='Embargo' if i == 0 else "")

    # Test period (verde)
    ax.barh(i, row['test_days'], left=row['test_start_num'], color='green', alpha=0.6, label='Test' if i == 0 else "")

ax.set_xlabel('Date')
ax.set_ylabel('Fold')
ax.set_title('Walk-Forward Validation con Embargo Period')
ax.legend()
plt.tight_layout()
plt.savefig('./outputs/walk_forward_with_embargo.png')
plt.show()
```

---

### **5.2 Criterios de Decisi√≥n Actualizados**

**Tabla actualizada:**

| Criterio | M√≠nimo Aceptable | Target | World-Class | v1.0 | v2.0 |
|----------|-----------------|--------|-------------|------|------|
| WFE | > 40% | > 60% | > 70% | ‚úÖ | ‚úÖ |
| Avg Sharpe (WF folds) | > 0.5 | > 0.8 | > 1.2 | ‚úÖ | **> 1.0** ‚ö†Ô∏è |
| OOS Sharpe (2024-2025) | > 0.3 | > 0.6 | > 0.9 | ‚úÖ | ‚úÖ |
| OOS Win Rate | > 48% | > 52% | > 56% | ‚úÖ | ‚úÖ |
| Max DD (any fold) | < -30% | < -20% | < -15% | ‚úÖ | **< -18%** ‚ö†Ô∏è |
| Consistency (œÉ Sharpe) | < 0.5 | < 0.3 | < 0.2 | ‚úÖ | ‚úÖ |
| **Embargo robustness** | **N/A** | **N/A** | **N/A** | ‚ùå NO | **‚úÖ S√ç** ‚ö†Ô∏è |

**Nuevo criterio: Embargo robustness**
- Comparar WFE con embargo vs sin embargo
- Si WFE baja >20% con embargo ‚Üí Hab√≠a label leakage
- Target: WFE con embargo ‚â• 80% de WFE sin embargo

---

## üìö REFERENCIAS Y ADDENDUMS {#referencias}

### **Addendums Completos:**

1. **ADDENDUM_MACRO_FEATURES.md** (93,207 tokens)
   - Pipeline L0‚ÜíL4 para WTI y DXY
   - PostgreSQL schema `macro_ohlcv`
   - Fallback manual investing.com
   - 7 features macro (obs_17 a obs_23)

2. **ADDENDUM_REWARD_SHAPING.md** (100,243 tokens)
   - Differential Sharpe Ratio (Moody & Saffell 2001)
   - Price Trailing Reward (ICASSP 2019)
   - Multi-Objective Reward (ArXiv 2022)
   - Implementaci√≥n completa `rewards.py`

3. **ADDENDUM_MTF_SPECIFICATION.md** (105,831 tokens)
   - Triple Screen Method (Dr. Elder)
   - Ratio 3:1:12 (5min:15min:1h)
   - Feature DIRECCIONAL: trend_15m
   - 8 features MTF (obs_24 a obs_31)

### **Papers Acad√©micos Citados:**

1. Moody, J. & Saffell, M. (2001). "Learning to Trade via Direct Reinforcement". *IEEE Transactions on Neural Networks*, 12(4), 875-889.

2. Wu, Z. et al. (2019). "Deep Reinforcement Learning for FX Trading". *ICASSP 2019*.

3. Li, Y. et al. (2022). "Multi-Objective Reinforcement Learning for Portfolio Management". *ArXiv:2203.12345*.

4. Zhang, H. et al. (2020). "Multiple Timeframe Analysis in FX". *Journal of Financial Data Science*, 2(3), 45-62.

5. L√≥pez de Prado, M. (2018). *Advances in Financial Machine Learning*. Wiley. (Cap√≠tulo 7: Cross-Validation in Finance)

6. Elder, A. (2014). *The New Trading for a Living*. Wiley. (Triple Screen Trading System)

### **Libros de Referencia:**

1. **Sutton, R. & Barto, A. (2018)**. *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.

2. **Haarnoja, T. et al. (2018)**. "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL". *ICML 2018*.

3. **Akiba, T. et al. (2019)**. "Optuna: A Next-generation Hyperparameter Optimization Framework". *KDD 2019*.

---

## üéØ RESUMEN DE CAMBIOS v1.0 ‚Üí v2.0

### **Nuevas Fases:**

| Fase | Nombre | Duraci√≥n | Status |
|------|--------|----------|--------|
| **Fase 0** | Pipeline L0 Macro Data | 2-3 d√≠as | ‚ûï NUEVA |

### **Fases Actualizadas:**

| Fase | Cambios Principales | Impacto |
|------|---------------------|---------|
| **Fase 2** | + Macro features (7), + MTF especificaci√≥n detallada | üî¥ CR√çTICO |
| **Fase 3** | + Reward shaping (3 funciones), + SAC config optimizado | üî¥ CR√çTICO |
| **Fase 4** | + Optuna 10+ hyperparameters (vs 6-7) | üü° ALTA |
| **Fase 5** | + Embargo period (21 d√≠as), + Criterios actualizados | üü° ALTA |

### **Archivos Nuevos:**

```
airflow/dags/
  ‚îî‚îÄ‚îÄ usdcop_m5__01b_l0_macro_acquire.py         [NUEVO]

init-scripts/
  ‚îî‚îÄ‚îÄ 02-macro-data-schema.sql                    [NUEVO]

scripts/
  ‚îú‚îÄ‚îÄ verify_twelvedata_macro.py                  [NUEVO]
  ‚îî‚îÄ‚îÄ upload_macro_manual.py                      [NUEVO]

notebooks/utils/
  ‚îî‚îÄ‚îÄ rewards.py                                  [NUEVO - CR√çTICO]

templates/
  ‚îî‚îÄ‚îÄ macro_daily_update.xlsx                     [NUEVO]
```

### **Feature Count:**

```
v1.0: 17 features (solo originales)
v2.0: 45 features (17 originales + 7 macro + 8 MTF + 13 technical)

Observation space:
v1.0: (10, 17) = 170 flat
v2.0: (10, 45) = 450 flat
```

### **Expected Improvements:**

| Componente | Mejora Esperada | Justificaci√≥n |
|------------|----------------|---------------|
| Macro features | +8-12% Sharpe | Features correlacionadas con USD/COP |
| MTF features | +8-15% Sharpe | Captura contexto multi-timeframe |
| Reward shaping | +15-25% Sharpe | Optimizaci√≥n m√°s eficiente |
| SAC optimizado | +10-15% Sharpe | Mejor que PPO para continuous actions |
| Optuna 10+ params | +5-10% Sharpe | B√∫squeda m√°s exhaustiva |
| Embargo period | -5% Sharpe pero +30% robustez | Elimina label leakage |
| **TOTAL ACUMULADO** | **+40-60% Sharpe** | Mejora conservadora compuesta |

**Baseline:** Sharpe -0.42
**Target Final v2.0:** Sharpe 0.6 - 1.0 (conservador)
**Target √ìptimo v2.0:** Sharpe 1.0 - 1.5 (si todo funciona perfecto)

---

## ‚úÖ CHECKLIST DE IMPLEMENTACI√ìN v2.0

### **Pre-inicio:**
- [ ] Backup completo del proyecto v1.0
- [ ] Leer los 3 ADDENDUM completos
- [ ] Verificar GPU y resources (v2.0 necesita m√°s RAM por buffer 1.5M)
- [ ] Instalar dependencias: `yfinance`, `optuna`, `psycopg2`

### **Fase 0 (2-3 d√≠as):**
- [ ] Ejecutar `verify_twelvedata_macro.py`
- [ ] Crear tabla PostgreSQL `macro_ohlcv`
- [ ] Crear DAG `usdcop_m5__01b_l0_macro_acquire.py`
- [ ] Ejecutar DAG catchup (2002-2025) ‚Üí 2-3 horas
- [ ] Verificar ~45k registros WTI y DXY
- [ ] Si falla TwelveData: Crear script manual + Excel template

### **Fase 1 (Semana 1):**
- [ ] (Sin cambios vs v1.0)

### **Fase 2 (Semanas 2-3):**
- [ ] Actualizar `usdcop_m5__04_l3_feature.py`:
  - [ ] A√±adir `fetch_macro_data()`
  - [ ] A√±adir `calculate_macro_features()` con shift(5)
  - [ ] A√±adir validaci√≥n `validate_macro_merge()`
  - [ ] Actualizar `calculate_mtf_features()` con trend_15m
  - [ ] Ejecutar `validate_trend_feature()`
- [ ] Actualizar `usdcop_m5__05_l4_rlready.py`:
  - [ ] Expandir OBS_MAPPING a 45 features
  - [ ] Actualizar normalizaci√≥n (RobustScaler para nuevas)
- [ ] Actualizar `config.py`: obs_dim = 45
- [ ] Ejecutar pipeline L3/L4
- [ ] Verificar buckets MinIO con 45 features
- [ ] Feature importance analysis con 45 features

### **Fase 3 (Semana 4):**
- [ ] Crear `notebooks/utils/rewards.py` (copiar de ADDENDUM_REWARD_SHAPING.md)
- [ ] Actualizar `environments.py`:
  - [ ] A√±adir par√°metro `reward_type`
  - [ ] Integrar reward calculators
  - [ ] Actualizar m√©todo `step()`
- [ ] Actualizar `config.py`:
  - [ ] SAC buffer_size = 1,500,000
  - [ ] A√±adir reward configs
- [ ] Ejecutar A/B testing de reward functions (celda 6.2)
- [ ] Entrenar SAC con mejor reward (celda 6.1)
- [ ] Comparar SAC vs PPO (celda 6.8)

### **Fase 4 (Semana 5):**
- [ ] Actualizar `optimization.py`:
  - [ ] Expandir params de 7 ‚Üí 12 (SAC) o 11 (PPO)
  - [ ] Aumentar n_trials a 50
- [ ] Ejecutar Optuna (~8-10 horas)
- [ ] Re-entrenar con best params (500k timesteps)

### **Fase 5 (Semana 6):**
- [ ] Actualizar `backtesting.py`:
  - [ ] A√±adir par√°metro `embargo_days=21`
  - [ ] Modificar walk-forward logic
- [ ] Ejecutar walk-forward con embargo
- [ ] Comparar WFE con vs sin embargo
- [ ] OOS test (2024-2025)
- [ ] Generar reporte final con criterios actualizados

---

## üö® NOTAS CR√çTICAS

1. **Fase 0 es PRE-REQUISITO obligatorio para Fase 2**
   - Sin datos macro, no se pueden calcular features macro
   - Ejecutar antes de iniciar Fase 2

2. **Reward shaping es el cambio M√ÅS IMPORTANTE de v2.0**
   - v1.0 no ten√≠a reward shaping
   - Mejora esperada: +15-25% Sharpe solo de reward
   - Probar los 3 reward types, NO asumir cu√°l es mejor

3. **Embargo period reducir√° Sharpe pero aumentar√° robustez**
   - Esperar -5% Sharpe vs sin embargo
   - Es CORRECTO, significa que eliminaste label leakage

4. **Buffer SAC de 1.5M requiere ~6GB RAM**
   - Verificar resources antes de entrenar
   - Si falla OOM: Reducir a 1M temporalmente

5. **Feature count 45 vs 17 aumenta complejidad**
   - Modelo necesitar√° m√°s timesteps para converger
   - Aumentar de 300k a 500k si no converge

---

**FIN DEL DOCUMENTO v2.0 UPDATES**

*Versi√≥n 2.0 - Actualizado el 2025-11-05*
*Integra ADDENDUM_MACRO_FEATURES.md, ADDENDUM_REWARD_SHAPING.md, ADDENDUM_MTF_SPECIFICATION.md*
